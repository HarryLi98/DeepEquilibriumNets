{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Equilibrium Nets\n",
    "\n",
    "The aim is to use TensorFlow (v2.6.0) to solve a simple model 3-generation model with deep equilibrium nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "# Setting up\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.initializers import HeNormal, HeUniform, GlorotNormal, GlorotUniform\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further setting up\n",
    "\n",
    "# Set the seed for replicable results\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SCALAR PARAMETERS\n",
    "alpha = 0.1\n",
    "beta = 0.99**25\n",
    "gamma = 3\n",
    "J = 3\n",
    "jret = 2\n",
    "thetaret = 0.2\n",
    "delta = (1 + 0.015)**25 - 1\n",
    "Gmu = (1 + 0.005)**25\n",
    "Gchi = (1 + 0.02)**25\n",
    "Pic = (1 + 0.02)**25\n",
    "Hs = 0.79/5\n",
    "\n",
    "# COMPUTE PARAMETER TRANSFORMATIONS\n",
    "betahat = Gchi**((1-alpha)*(1-gamma))*beta\n",
    "Pih = Gchi * Pic\n",
    "\n",
    "# DEFINE AGE-VARYING PRODUCTIVITY AND SURVIVAL RATES\n",
    "chi1 = 0.874264\n",
    "mu1 = 0.493497\n",
    "\n",
    "chi = chi1 * np.array([1, 1.5])\n",
    "\n",
    "zeta = np.array([0.976163, 0.784336, 0])\n",
    "zeta_0 = np.concatenate(([1], zeta[0:2]))\n",
    "\n",
    "# COMPUTE POPULATION DISTRIBUTION\n",
    "mu = np.zeros(J)\n",
    "mu[0] = mu1\n",
    "for j in range(1, J):\n",
    "    mu[j] = (zeta[j-1] / Gmu) * mu[j-1]\n",
    "\n",
    "# COMPUTE TAX RATE\n",
    "tau = np.sum([thetaret*Gchi**(jret - j)*mu[j] for j in range(jret, J)]) / np.sum(mu[0:jret]*chi)\n",
    "\n",
    "# COMPUTE AGE-VARYING INCOME\n",
    "y = np.zeros(J)\n",
    "for j in range(J):\n",
    "    if j < jret:\n",
    "        y[j] = (1-tau) * chi[j]\n",
    "    else:\n",
    "        y[j] = thetaret * Gchi ** (jret-j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20000\n",
    "len_episodes = 32\n",
    "epochs_per_episode = 4\n",
    "minibatch_size = 8\n",
    "num_minibatches = int(len_episodes / minibatch_size)\n",
    "lr = 5e-5\n",
    "\n",
    "#Â Neural network architecture parameters\n",
    "num_input_nodes = 2 + 3*J  # Dimension of extended state space (4 aggregate quantities and 3 distributions)\n",
    "num_hidden_nodes = [100, 50]  # Dimension of hidden layers\n",
    "num_output_nodes = 2*J + 2  # Output dimension: 3 for housing, 3 for consumption, 2 aggregate quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network\n",
    "model = Sequential([\n",
    "    Dense(100, activation='swish', input_shape=(num_input_nodes,), kernel_initializer=GlorotNormal),\n",
    "    Dense(50, activation='swish'),\n",
    "    Dense(num_output_nodes, activation='softplus')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               1200      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,658\n",
      "Trainable params: 6,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Economic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A. Current period (t) <a id='currentperiod'></a>\n",
    "\n",
    "Using the current extended state $\\bm{X}$, we can calculate the economy. The state is composed of today's tangible wealth. Note that this constitutes the minimal state. Often, including redundant variables is a simple way to increase the speed of convergence (see the [working paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3393482) for more information on this).\n",
    "\n",
    "Using the current extended state $\\bm{X}$, we predict how much the agents save in financial assets and how much housing they consume. We also predict house prices and interest rates. We do this by passing the state $\\bm{X}$ to the neural network.\n",
    "\n",
    "Then we can calculate the agents' consumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random economically feasible starting point\n",
    "X = tf.random.uniform(shape=(1, num_input_nodes), minval=0, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def unpack_output(output):\n",
    "    \"\"\"\n",
    "    Unpacks the output of the neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : tensor\n",
    "        Output of neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack simulated housing and financial assets\n",
    "    R = tf.expand_dims(output[:, 0], axis=1)\n",
    "    ph = tf.expand_dims(output[:, 1], axis=1)\n",
    "    c = output[:, 2:5]\n",
    "    h_out = output[:, 5:8]\n",
    "\n",
    "    return R, ph, c, h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack_output(model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def compute_economy(X, output):\n",
    "    \"\"\"\n",
    "    Solves today's economy given state and output of neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack inputs and outputs\n",
    "    w = tf.cast(X[:, 2:5], dtype=tf.float32) # Distribution of tangible wealth\n",
    "    R, ph, c, h_out = unpack_output(output)\n",
    "\n",
    "    # Compute savings for oldest cohort\n",
    "    a_out_J = -(Pih * ph * tf.expand_dims(h_out[:, J-1], 1)) / R\n",
    "    # Compute savings for other cohorts\n",
    "    a_out = w[:, 0:J-1] - c[:, 0:J-1] - ph*(1+delta)*h_out[:, 0:J-1]\n",
    "    a_out_all = tf.concat([a_out, a_out_J], axis=1)\n",
    "\n",
    "    # Compute aggregate savings and housing\n",
    "    A_out = tf.reduce_sum(mu * a_out_all, axis=1, keepdims=True)\n",
    "    H_out = tf.reduce_sum(mu * h_out, axis=1, keepdims=True)\n",
    "\n",
    "    return R, ph, a_out, a_out_all, h_out, c, A_out, H_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_economy(X, model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def compute_next_state(X, output):\n",
    "    \"\"\"\n",
    "    Computes the state for the next period, given the outputs of the neural network today.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs : tensors\n",
    "        Outputs of neural network\n",
    "    \"\"\"\n",
    "\n",
    "    R, ph, a_out, a_out_all, h_out, c, A_out, H_out = compute_economy(X, output)\n",
    "\n",
    "    m = tf.shape(X)[0]\n",
    "\n",
    "    a_prime = tf.concat([tf.zeros([m, 1]), a_out], axis=1)\n",
    "    h_prime = tf.concat([tf.zeros([m, 1]), h_out[:, 0:J-1]], axis=1)\n",
    "\n",
    "    # Distribution of tomorrow's financial wealth\n",
    "    w_prime = y + (R*a_prime + Pih*ph*h_prime) / (zeta_0*Gchi*Pic)\n",
    "\n",
    "    # Tomorrow's extended state: Concatenate the parts together\n",
    "    X_prime = tf.concat([\n",
    "        # A_out,\n",
    "        # H_out,\n",
    "        R,\n",
    "        ph,\n",
    "        w_prime,\n",
    "        a_prime,\n",
    "        h_prime\n",
    "        ], axis=1)        \n",
    "\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_next_state(X, model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def run_period(X):\n",
    "    \"\"\"\n",
    "    Evaluate neural network given today's state and generate tomorrow's state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : tensor\n",
    "        Today's extended state\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        Tomorrow's extended state\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate neural network\n",
    "    output = model(X)\n",
    "\n",
    "    # Compute next state\n",
    "    X_prime = compute_next_state(X, output)\n",
    "    \n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_period(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def simulate(X):\n",
    "    \"\"\"\n",
    "    Simulate and return euler errors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        Euler errors\n",
    "    \"\"\"\n",
    "\n",
    "    m = tf.shape(X)[0]\n",
    "\n",
    "    # --- Evaluate neural network given today's state ---\n",
    "    output = model(X)\n",
    "    tf.debugging.assert_all_finite(output, \"Prediction not finite\")\n",
    "\n",
    "    # --- Solve economy ---\n",
    "    w = tf.cast(X[:, 2:5], dtype=tf.float32) # Distribution of tangible wealth\n",
    "    R, ph, c, h_out = unpack_output(output)\n",
    "\n",
    "    # Compute savings for oldest cohort\n",
    "    a_out_J = -(Pih * ph * tf.expand_dims(h_out[:, J-1], 1)) / R\n",
    "    # Compute savings for other cohorts\n",
    "    a_out = w[:, 0:J-1] - c[:, 0:J-1] - ph*(1+delta)*h_out[:, 0:J-1]\n",
    "    a_out_all = tf.concat([a_out, a_out_J], axis=1)\n",
    "\n",
    "    # Compute aggregate savings and housing\n",
    "    A_out = tf.reduce_sum(mu * a_out_all, axis=1, keepdims=True)\n",
    "    H_out = tf.reduce_sum(mu * h_out, axis=1, keepdims=True)\n",
    "\n",
    "    # --- Compute state tomorrow ---\n",
    "    a_prime = tf.concat([tf.zeros([m, 1]), a_out], axis=1)\n",
    "    h_prime = tf.concat([tf.zeros([m, 1]), h_out[:, 0:J-1]], axis=1)\n",
    "\n",
    "    # Distribution of tomorrow's financial wealth\n",
    "    w_prime = y + (R*a_prime + Pih*ph*h_prime) / (zeta_0*Gchi*Pic)\n",
    "\n",
    "    # Tomorrow's extended state: Concatenate the parts together\n",
    "    X_prime = tf.concat([\n",
    "        # A_out,\n",
    "        # H_out,\n",
    "        R,\n",
    "        ph,\n",
    "        w_prime,\n",
    "        a_prime,\n",
    "        h_prime\n",
    "        ], axis=1)\n",
    "\n",
    "    # --- Evaluate neural network given tomorrow's state ---\n",
    "    output_prime = model(X_prime)\n",
    "\n",
    "    # --- Solve tomorrow's economy ---\n",
    "    w_prime = tf.cast(X_prime[:, 2:5], dtype=tf.float32) # Distribution of tangible wealth\n",
    "    R_prime, ph_prime, c_prime, h_prime_out = unpack_output(output_prime)\n",
    "\n",
    "    # Compute savings for oldest cohort\n",
    "    a_prime_out_J = -(Pih * ph_prime * tf.expand_dims(h_prime_out[:, J-1], 1)) / R_prime\n",
    "    a_prime_out = w_prime[:, 0:J-1] - c_prime[:, 0:J-1] - ph_prime*(1+delta)*h_prime_out[:, 0:J-1]\n",
    "    a_prime_out_all = tf.concat([a_prime_out, a_prime_out_J], axis=1)\n",
    "\n",
    "    # Compute aggregate savings and housing\n",
    "    A_prime_out = tf.reduce_sum(mu * a_prime_out_all, axis=1, keepdims=True)\n",
    "    H_prime_out = tf.reduce_sum(mu * h_prime_out, axis=1, keepdims=True)\n",
    "\n",
    "    # --- Compute euler errors and other equilibrium conditions ---\n",
    "    # Euler equation 1 (Intertemporal)\n",
    "    opt_euler1 = -1 + (c_prime[:, 1:J] / c[:, 0:J-1])*(R*betahat/(Gchi*Pic))**(-1/gamma)\n",
    "\n",
    "    # Euler equation 2 (Intratemporal)\n",
    "    opt_euler2 = -1 + (c/h_out)*(((1-alpha)/alpha)*ph*(1+delta-Pih/R))**(-1)\n",
    "    # opt_euler_intra = -1 + (c/h_out)*(((1-alpha)/alpha)*ph*(1+delta-Pih/R))**(-1)\n",
    "    # opt_euler_intra_prime = -1 + (c_prime/h_prime_out)*(((1-alpha)/alpha)*ph_prime*(1+delta-Pih/R_prime))**(-1)\n",
    "    # opt_euler2 = tf.concat([opt_euler_intra, opt_euler_intra_prime], axis=1)\n",
    "\n",
    "    # Budget constraint for eldest\n",
    "    opt_budget = w[:, J-1] - c[:, J-1] - ph*(1+delta)*h_out[:, J-1] - a_out_J\n",
    "    opt_budget_prime = w_prime[:, J-1] - c_prime[:, J-1] - ph_prime*(1+delta)*h_prime_out[:, J-1] - a_prime_out_J\n",
    "    opt_euler3 = tf.concat([opt_budget, opt_budget_prime], axis=1)\n",
    "\n",
    "    # Market clearing condition for aggregate savings\n",
    "    # opt_market_a = A_out\n",
    "    # opt_market_a_prime = A_prime_out\n",
    "    # opt_market_A = tf.concat([opt_market_a, opt_market_a_prime], axis=1)\n",
    "    opt_market_A = A_out\n",
    "\n",
    "    # Market clearing condition for aggregate housing\n",
    "    # opt_market_h = H_out - Hs\n",
    "    # opt_market_h_prime = H_prime_out - Hs\n",
    "    # opt_market_H = tf.concat([opt_market_h, opt_market_h_prime], axis=1)\n",
    "    opt_market_H = H_out - Hs\n",
    "\n",
    "    # Concatenate the equilibrium functions\n",
    "    combined_opt = [opt_euler1, opt_euler2, opt_euler3, opt_market_A, opt_market_H] # Dimensions: [2, 6, 2, 2, 2]\n",
    "    opt_predict = tf.concat(combined_opt, axis=1)\n",
    "\n",
    "    # Check if Euler errors are valid\n",
    "    tf.debugging.assert_all_finite(opt_predict, \"Euler errors not finite\")\n",
    "\n",
    "    # Define the \"correct\" outputs. For all equilibrium functions, the correct outputs is zero.\n",
    "    opt_correct = tf.zeros_like(opt_predict)\n",
    "\n",
    "    return opt_predict, opt_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 9), dtype=float32, numpy=\n",
       " array([[ 1.13403,  1.58465, -1.08302, -1.06671, -1.07483,  0.55627,\n",
       "          0.99124, -1.15189,  0.68716]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 9), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "simulate(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "Next, we chose an optimizer; i.e., the algorithm we use to perform gradient descent. We use [Adam](https://arxiv.org/abs/1412.6980), a favorite in deep learning research. Adam uses a parameter specific learning rate and momentum, which encourages gradient descent steps that occur in a consistent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the _deep equilibrium net_\n",
    "\n",
    "Now we can begin training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each episode, we simulate the training dataset, which is then used to train the neural network. The training set is split into batches, which is fed into the neural network. In each epoch, we pass through the entire training set once. The training lasts for epochs_per_episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random economically feasible starting point\n",
    "x_start = tf.random.uniform(shape=(1, num_input_nodes), minval=0, maxval=1)\n",
    "\n",
    "# Adam optimizer\n",
    "mse = MeanSquaredError()\n",
    "optimizer = Adam(learning_rate=lr, amsgrad=True, clipvalue=1.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    loss='mse'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episodes(x_start, episode_length):\n",
    "    \"\"\"Simulate an episode for a given starting point using the current\n",
    "       neural network state.\n",
    "\n",
    "    Args:\n",
    "        x_start: Starting state to simulate forward from,\n",
    "        episode_length: Number of steps to simulate forward,\n",
    "\n",
    "    Returns:\n",
    "        X_episodes: Tensor of states [z, k] to train on (training set).\n",
    "    \"\"\"\n",
    "    \n",
    "    dim_state = np.shape(x_start)[1]\n",
    "\n",
    "    # Initialise empty array of episodes\n",
    "    X_episodes = np.zeros([episode_length, dim_state])\n",
    "    X_episodes[0, :] = x_start\n",
    "    X_old = x_start\n",
    "\n",
    "    for t in range(1, episode_length):\n",
    "        X_new = run_period(X_old)\n",
    "        \n",
    "        # Append it to the dataset\n",
    "        X_episodes[t, :] = X_new\n",
    "        X_old = X_new\n",
    "\n",
    "    return X_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate_episodes(x_start, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define full training algorithm\n",
    "# Which alternates between simulating episodes (valid states) and training_step()\n",
    "\n",
    "@tf.function\n",
    "def train_step(X_train):\n",
    "\n",
    "    X_train = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "    X_train = X_train.shuffle(buffer_size=len_episodes).batch(minibatch_size)\n",
    "    loss_value = np.inf\n",
    "\n",
    "    # add loop for epochs here\n",
    "    for epoch in range(epochs_per_episode):\n",
    "\n",
    "        # add loop for batches here - maybe manually batch\n",
    "        for X_batch in X_train:\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # Forward pass of the model to get Euler errors\n",
    "                opt_predict, opt_correct = simulate(X_batch)\n",
    "\n",
    "                # Compute loss value\n",
    "                loss_value = mse(opt_correct, opt_predict)               \n",
    "\n",
    "            # Use gradient tape to retrieve gradients of trainable variables with respect to loss\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            # Run one step of gradient descent by updating the value of the variables to minimise the loss\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_step(x_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "\n",
    "# @tf.function\n",
    "def training_algorithm(x_start):\n",
    "\n",
    "    num_episodes = 10000 #try 30000\n",
    "    loss_list = np.empty(num_episodes)\n",
    "\n",
    "    # Training Loops\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Simulate episodes\n",
    "        X_episodes = simulate_episodes(x_start, len_episodes)\n",
    "\n",
    "        # Train model\n",
    "        loss_value = train_step(X_episodes)\n",
    "\n",
    "        # Update starting episode\n",
    "        x_start = X_episodes[-1, :].reshape([1, -1])\n",
    "\n",
    "        # Store losses, euler errors\n",
    "        loss_list[episode] = float(tf.math.log(loss_value))\n",
    "        \n",
    "        # Log\n",
    "        if episode % 10 == 0:\n",
    "            tf.print(f\"Episode {episode}: log10(loss): {float(tf.math.log(loss_value)):.5f}\")\n",
    "            print(x_start)\n",
    "          \n",
    "    return x_start, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: log10(loss): 0.27913\n",
      "[[ 0.65457  0.67433  0.82221  1.59074  0.85781  0.      -0.59808  0.30065\n",
      "   0.       0.7331   0.6567 ]]\n",
      "Episode 10: log10(loss): -0.73541\n",
      "[[ 0.84258  0.61735  0.82221  1.51163  0.76921  0.      -0.68269  0.29523\n",
      "   0.       0.78624  0.57347]]\n",
      "Episode 20: log10(loss): -1.12014\n",
      "[[ 0.95847  0.60098  0.82221  1.43166  0.68661  0.      -0.78604  0.20558\n",
      "   0.       0.78791  0.51326]]\n",
      "Episode 30: log10(loss): -1.21748\n",
      "[[ 1.00998  0.59908  0.82221  1.36749  0.62641  0.      -0.85474  0.12615\n",
      "   0.       0.754    0.47925]]\n",
      "Episode 40: log10(loss): -1.24888\n",
      "[[ 1.01043  0.5976   0.82221  1.33362  0.5947   0.      -0.88661  0.08651\n",
      "   0.       0.7208   0.46369]]\n",
      "Episode 50: log10(loss): -1.26771\n",
      "[[ 0.99086  0.59487  0.82221  1.31683  0.57685  0.      -0.90454  0.06827\n",
      "   0.       0.69682  0.45463]]\n",
      "Episode 60: log10(loss): -1.28265\n",
      "[[ 0.96681  0.59185  0.82221  1.30669  0.56434  0.      -0.9216   0.05934\n",
      "   0.       0.68035  0.44682]]\n",
      "Episode 70: log10(loss): -1.29568\n",
      "[[ 0.94233  0.58908  0.82221  1.29979  0.55441  0.      -0.94047  0.05554\n",
      "   0.       0.6691   0.43888]]\n",
      "Episode 80: log10(loss): -1.30740\n",
      "[[ 0.91828  0.58671  0.82221  1.29507  0.54609  0.      -0.96088  0.05529\n",
      "   0.       0.66147  0.43051]]\n",
      "Episode 90: log10(loss): -1.31812\n",
      "[[ 0.89479  0.58478  0.82221  1.29203  0.53891  0.      -0.98211  0.05782\n",
      "   0.       0.65632  0.4217 ]]\n",
      "Episode 100: log10(loss): -1.32802\n",
      "[[ 0.87186  0.58329  0.82221  1.29037  0.53262  0.      -1.00352  0.06259\n",
      "   0.       0.65277  0.4125 ]]\n",
      "Episode 110: log10(loss): -1.33722\n",
      "[[ 0.84946  0.58227  0.82221  1.28986  0.527    0.      -1.02464  0.06921\n",
      "   0.       0.65015  0.40296]]\n",
      "Episode 120: log10(loss): -1.34582\n",
      "[[ 0.82754  0.58173  0.82221  1.29028  0.52191  0.      -1.04521  0.07732\n",
      "   0.       0.64799  0.39316]]\n",
      "Episode 130: log10(loss): -1.35391\n",
      "[[ 0.80607  0.58171  0.82221  1.29148  0.51723  0.      -1.06508  0.08661\n",
      "   0.       0.64594  0.38315]]\n",
      "Episode 140: log10(loss): -1.36154\n",
      "[[ 0.78502  0.58224  0.82221  1.29334  0.51286  0.      -1.08426  0.09682\n",
      "   0.       0.64375  0.37296]]\n",
      "Episode 150: log10(loss): -1.36878\n",
      "[[ 0.76435  0.58336  0.82221  1.29574  0.50872  0.      -1.10282  0.10776\n",
      "   0.       0.64129  0.36262]]\n",
      "Episode 160: log10(loss): -1.37567\n",
      "[[ 0.74405  0.58513  0.82221  1.2986   0.50476  0.      -1.12089  0.11928\n",
      "   0.       0.63845  0.35216]]\n",
      "Episode 170: log10(loss): -1.38225\n",
      "[[ 0.72408  0.58756  0.82221  1.30187  0.50092  0.      -1.13859  0.13128\n",
      "   0.       0.63519  0.34158]]\n",
      "Episode 180: log10(loss): -1.38855\n",
      "[[ 0.70442  0.59072  0.82221  1.30552  0.49716  0.      -1.15604  0.1437\n",
      "   0.       0.63148  0.33089]]\n",
      "Episode 190: log10(loss): -1.39460\n",
      "[[ 0.68507  0.59462  0.82221  1.3095   0.49345  0.      -1.17337  0.15651\n",
      "   0.       0.6273   0.32009]]\n",
      "Episode 200: log10(loss): -1.40041\n",
      "[[ 0.66601  0.59933  0.82221  1.3138   0.48978  0.      -1.19065  0.16968\n",
      "   0.       0.62266  0.30918]]\n",
      "Episode 210: log10(loss): -1.40600\n",
      "[[ 0.64724  0.60488  0.82221  1.31841  0.48613  0.      -1.20795  0.18324\n",
      "   0.       0.61754  0.29817]]\n",
      "Episode 220: log10(loss): -1.41138\n",
      "[[ 0.62877  0.61133  0.82221  1.32333  0.48248  0.      -1.22534  0.19718\n",
      "   0.       0.61197  0.28708]]\n",
      "Episode 230: log10(loss): -1.41656\n",
      "[[ 0.61062  0.61874  0.82221  1.32855  0.47883  0.      -1.24282  0.21151\n",
      "   0.       0.60592  0.2759 ]]\n",
      "Episode 240: log10(loss): -1.42154\n",
      "[[ 0.59279  0.62717  0.82221  1.33407  0.47518  0.      -1.26042  0.22625\n",
      "   0.       0.59942  0.26468]]\n",
      "Episode 250: log10(loss): -1.42633\n",
      "[[ 0.57532  0.63671  0.82221  1.33988  0.47152  0.      -1.27813  0.24139\n",
      "   0.       0.59246  0.25344]]\n",
      "Episode 260: log10(loss): -1.43092\n",
      "[[ 0.55823  0.64743  0.82221  1.34599  0.46787  0.      -1.29596  0.25693\n",
      "   0.       0.58502  0.2422 ]]\n",
      "Episode 270: log10(loss): -1.43533\n",
      "[[ 0.54153  0.65946  0.82221  1.35239  0.46422  0.      -1.31389  0.27287\n",
      "   0.       0.57709  0.23101]]\n",
      "Episode 280: log10(loss): -1.43955\n",
      "[[ 0.52524  0.67292  0.82221  1.35907  0.46059  0.      -1.33189  0.28918\n",
      "   0.       0.56865  0.21987]]\n",
      "Episode 290: log10(loss): -1.44359\n",
      "[[ 0.50938  0.68795  0.82221  1.36603  0.45698  0.      -1.34996  0.30585\n",
      "   0.       0.55967  0.20884]]\n",
      "Episode 300: log10(loss): -1.44744\n",
      "[[ 0.49398  0.70473  0.82221  1.37326  0.4534   0.      -1.36806  0.32286\n",
      "   0.       0.55011  0.19794]]\n",
      "Episode 310: log10(loss): -1.45111\n",
      "[[ 0.47903  0.72347  0.82221  1.38075  0.44985  0.      -1.38618  0.34017\n",
      "   0.       0.53993  0.18719]]\n",
      "Episode 320: log10(loss): -1.45460\n",
      "[[ 0.46456  0.74443  0.82221  1.3885   0.44636  0.      -1.40427  0.35776\n",
      "   0.       0.52906  0.17662]]\n",
      "Episode 330: log10(loss): -1.45791\n",
      "[[ 0.45056  0.76793  0.82221  1.39648  0.44292  0.      -1.42233  0.37562\n",
      "   0.       0.51745  0.16623]]\n",
      "Episode 340: log10(loss): -1.46106\n",
      "[[ 0.43702  0.79435  0.82221  1.4047   0.43953  0.      -1.44033  0.39371\n",
      "   0.       0.50501  0.15604]]\n",
      "Episode 350: log10(loss): -1.46404\n",
      "[[ 0.42394  0.8242   0.82221  1.41314  0.43621  0.      -1.45824  0.41204\n",
      "   0.       0.49164  0.14605]]\n",
      "Episode 360: log10(loss): -1.46687\n",
      "[[ 0.41129  0.85814  0.82221  1.42179  0.43295  0.      -1.47607  0.43058\n",
      "   0.       0.47722  0.13624]]\n",
      "Episode 370: log10(loss): -1.46955\n",
      "[[ 0.39903  0.89704  0.82221  1.43064  0.42974  0.      -1.49378  0.44936\n",
      "   0.       0.4616   0.12661]]\n",
      "Episode 380: log10(loss): -1.47211\n",
      "[[ 0.38713  0.94208  0.82221  1.43967  0.42656  0.      -1.51138  0.46839\n",
      "   0.       0.44456  0.11712]]\n",
      "Episode 390: log10(loss): -1.47456\n",
      "[[ 0.37549  0.99495  0.82221  1.44888  0.42341  0.      -1.52885  0.48773\n",
      "   0.       0.42586  0.10773]]\n",
      "Episode 400: log10(loss): -1.47694\n",
      "[[ 0.36402  1.05807  0.82221  1.45824  0.42023  0.      -1.54615  0.50747\n",
      "   0.       0.40515  0.09839]]\n",
      "Episode 410: log10(loss): -1.47930\n",
      "[[ 0.35256  1.13517  0.82221  1.46772  0.41696  0.      -1.56325  0.52775\n",
      "   0.       0.38195  0.08901]]\n",
      "Episode 420: log10(loss): -1.48173\n",
      "[[ 0.34085  1.23226  0.82221  1.47723  0.41349  0.      -1.58002  0.54884\n",
      "   0.       0.35559  0.07948]]\n",
      "Episode 430: log10(loss): -1.48440\n",
      "[[ 0.32842  1.35979  0.82221  1.48664  0.40959  0.      -1.5962   0.57121\n",
      "   0.       0.32508  0.06964]]\n",
      "Episode 440: log10(loss): -1.48766\n",
      "[[ 0.31444  1.53752  0.82221  1.49547  0.40482  0.      -1.61112  0.59558\n",
      "   0.       0.28886  0.05923]]\n",
      "Episode 450: log10(loss): -1.49237\n",
      "[[ 0.29712  1.80817  0.82221  1.50255  0.39814  0.      -1.62284  0.62356\n",
      "   0.       0.24442  0.04788]]\n",
      "Episode 460: log10(loss): -1.50111\n",
      "[[ 0.27131  2.28802  0.82221  1.50467  0.38647  0.      -1.62592  0.66061\n",
      "   0.       0.1874   0.03482]]\n",
      "Episode 470: log10(loss): -1.52534\n",
      "[[ 0.21709  3.41891  0.82221  1.48974  0.3568   0.      -1.60086  0.73274\n",
      "   0.       0.11098  0.01869]]\n",
      "Episode 480: log10(loss): -1.63301\n",
      "[[ 7.59368e-02  7.24238e+00  8.22214e-01  1.43502e+00  2.64444e-01\n",
      "   0.00000e+00 -1.39302e+00  1.00752e+00  0.00000e+00  3.26121e-02\n",
      "   3.05433e-03]]\n",
      "Episode 490: log10(loss): -1.75164\n",
      "[[ 1.12020e-02  1.00458e+01  8.22214e-01  1.60518e+00  2.14102e-01\n",
      "   0.00000e+00 -1.23267e+00  1.43506e+00  0.00000e+00  3.66448e-02\n",
      "   5.06474e-04]]\n",
      "Episode 500: log10(loss): -1.79738\n",
      "[[ 2.18264e-03  1.06657e+01  8.22214e-01  2.38059e+00  2.03967e-01\n",
      "   0.00000e+00 -1.90250e+00  2.30981e+00  0.00000e+00  1.05147e-01\n",
      "   1.16088e-04]]\n",
      "Episode 510: log10(loss): -1.82213\n",
      "[[ 4.54967e-04  1.13543e+01  8.22214e-01  3.32605e+00  2.01035e-01\n",
      "   0.00000e+00 -2.79043e+00  3.30141e+00  0.00000e+00  1.79960e-01\n",
      "   2.23370e-05]]\n",
      "Episode 520: log10(loss): -1.82974\n",
      "[[ 2.34450e-04  1.13467e+01  8.22214e-01  3.81406e+00  2.00577e-01\n",
      "   0.00000e+00 -3.23898e+00  3.80156e+00  0.00000e+00  2.22048e-01\n",
      "   1.07228e-05]]\n",
      "Episode 530: log10(loss): -1.83319\n",
      "[[ 1.68600e-04  1.11875e+01  8.22214e-01  4.08796e+00  2.00427e-01\n",
      "   0.00000e+00 -3.50637e+00  4.08012e+00  0.00000e+00  2.49101e-01\n",
      "   7.09733e-06]]\n",
      "Episode 540: log10(loss): -1.83467\n",
      "[[ 1.54772e-04  1.08498e+01  8.22214e-01  4.22378e+00  2.00396e-01\n",
      "   0.00000e+00 -3.63437e+00  4.21787e+00  0.00000e+00  2.69071e-01\n",
      "   6.26962e-06]]\n",
      "Episode 550: log10(loss): -1.83546\n",
      "[[ 1.56430e-04  1.04730e+01  8.22214e-01  4.29988e+00  2.00400e-01\n",
      "   0.00000e+00 -3.70215e+00  4.29501e+00  0.00000e+00  2.85848e-01\n",
      "   6.15469e-06]]\n",
      "Episode 560: log10(loss): -1.83596\n",
      "[[ 1.62097e-04  1.01092e+01  8.22214e-01  4.35114e+00  2.00414e-01\n",
      "   0.00000e+00 -3.74713e+00  4.34696e+00  0.00000e+00  3.01084e-01\n",
      "   6.23202e-06]]\n",
      "Episode 570: log10(loss): -1.83632\n",
      "[[ 1.68974e-04  9.77025e+00  8.22214e-01  4.38815e+00  2.00430e-01\n",
      "   0.00000e+00 -3.77916e+00  4.38446e+00  0.00000e+00  3.15229e-01\n",
      "   6.37751e-06]]\n",
      "Episode 580: log10(loss): -1.83660\n",
      "[[ 1.75543e-04  9.46168e+00  8.22214e-01  4.41640e+00  2.00446e-01\n",
      "   0.00000e+00 -3.80326e+00  4.41311e+00  0.00000e+00  3.28425e-01\n",
      "   6.52416e-06]]\n",
      "Episode 590: log10(loss): -1.83683\n",
      "[[ 1.80847e-04  9.18615e+00  8.22214e-01  4.43902e+00  2.00458e-01\n",
      "   0.00000e+00 -3.82232e+00  4.43605e+00  0.00000e+00  3.40680e-01\n",
      "   6.63051e-06]]\n",
      "Episode 600: log10(loss): -1.83705\n",
      "[[ 1.84281e-04  8.94458e+00  8.22214e-01  4.45786e+00  2.00465e-01\n",
      "   0.00000e+00 -3.83803e+00  4.45519e+00  0.00000e+00  3.51938e-01\n",
      "   6.67085e-06]]\n",
      "Episode 610: log10(loss): -1.83727\n",
      "[[ 1.85521e-04  8.73684e+00  8.22214e-01  4.47409e+00  2.00467e-01\n",
      "   0.00000e+00 -3.85147e+00  4.47169e+00  0.00000e+00  3.62121e-01\n",
      "   6.63125e-06]]\n",
      "Episode 620: log10(loss): -1.83750\n",
      "[[ 1.84476e-04  8.56203e+00  8.22214e-01  4.48846e+00  2.00463e-01\n",
      "   0.00000e+00 -3.86335e+00  4.48629e+00  0.00000e+00  3.71152e-01\n",
      "   6.50802e-06]]\n",
      "Episode 630: log10(loss): -1.83774\n",
      "[[ 1.81263e-04  8.41859e+00  8.22214e-01  4.50138e+00  2.00454e-01\n",
      "   0.00000e+00 -3.87405e+00  4.49945e+00  0.00000e+00  3.78974e-01\n",
      "   6.30603e-06]]\n",
      "Episode 640: log10(loss): -1.83799\n",
      "[[ 1.76160e-04  8.30444e+00  8.22214e-01  4.51312e+00  2.00440e-01\n",
      "   0.00000e+00 -3.88378e+00  4.51139e+00  0.00000e+00  3.85561e-01\n",
      "   6.03720e-06]]\n",
      "Episode 650: log10(loss): -1.83826\n",
      "[[ 1.69541e-04  8.21711e+00  8.22214e-01  4.52378e+00  2.00423e-01\n",
      "   0.00000e+00 -3.89264e+00  4.52224e+00  0.00000e+00  3.90924e-01\n",
      "   5.71730e-06]]\n",
      "Episode 660: log10(loss): -1.83853\n",
      "[[ 1.61829e-04  8.15380e+00  8.22214e-01  4.53339e+00  2.00403e-01\n",
      "   0.00000e+00 -3.90060e+00  4.53204e+00  0.00000e+00  3.95109e-01\n",
      "   5.36412e-06]]\n",
      "Episode 670: log10(loss): -1.83881\n",
      "[[ 1.53452e-04  8.11151e+00  8.22214e-01  4.54190e+00  2.00382e-01\n",
      "   0.00000e+00 -3.90756e+00  4.54071e+00  0.00000e+00  3.98192e-01\n",
      "   4.99521e-06]]\n",
      "Episode 680: log10(loss): -1.83908\n",
      "[[ 1.44778e-04  8.08729e+00  8.22214e-01  4.54929e+00  2.00360e-01\n",
      "   0.00000e+00 -3.91345e+00  4.54823e+00  0.00000e+00  4.00274e-01\n",
      "   4.62506e-06]]\n",
      "Episode 690: log10(loss): -1.83935\n",
      "[[ 1.35952e-04  8.07938e+00  8.22214e-01  4.55597e+00  2.00337e-01\n",
      "   0.00000e+00 -3.91881e+00  4.55504e+00  0.00000e+00  4.01472e-01\n",
      "   4.25826e-06]]\n",
      "Episode 700: log10(loss): -1.83962\n",
      "[[ 1.26994e-04  8.08686e+00  8.22214e-01  4.56268e+00  2.00315e-01\n",
      "   0.00000e+00 -3.92463e+00  4.56187e+00  0.00000e+00  4.01910e-01\n",
      "   3.89462e-06]]\n",
      "Episode 710: log10(loss): -1.83988\n",
      "[[ 1.18208e-04  8.10682e+00  8.22214e-01  4.56917e+00  2.00292e-01\n",
      "   0.00000e+00 -3.93053e+00  4.56846e+00  0.00000e+00  4.01700e-01\n",
      "   3.54665e-06]]\n",
      "Episode 720: log10(loss): -1.84013\n",
      "[[ 1.09885e-04  8.13600e+00  8.22214e-01  4.57496e+00  2.00272e-01\n",
      "   0.00000e+00 -3.93582e+00  4.57433e+00  0.00000e+00  4.00952e-01\n",
      "   3.22523e-06]]\n",
      "Episode 730: log10(loss): -1.84036\n",
      "[[ 1.02106e-04  8.17245e+00  8.22214e-01  4.58005e+00  2.00252e-01\n",
      "   0.00000e+00 -3.94047e+00  4.57950e+00  0.00000e+00  3.99771e-01\n",
      "   2.93190e-06]]\n",
      "Episode 740: log10(loss): -1.84057\n",
      "[[ 9.49026e-05  8.21453e+00  8.22214e-01  4.58450e+00  2.00234e-01\n",
      "   0.00000e+00 -3.94451e+00  4.58402e+00  0.00000e+00  3.98250e-01\n",
      "   2.66646e-06]]\n",
      "Episode 750: log10(loss): -1.84078\n",
      "[[ 8.82591e-05  8.26106e+00  8.22214e-01  4.58844e+00  2.00217e-01\n",
      "   0.00000e+00 -3.94811e+00  4.58802e+00  0.00000e+00  3.96472e-01\n",
      "   2.42700e-06]]\n",
      "Episode 760: log10(loss): -1.84096\n",
      "[[ 8.21678e-05  8.31077e+00  8.22214e-01  4.59191e+00  2.00202e-01\n",
      "   0.00000e+00 -3.95127e+00  4.59154e+00  0.00000e+00  3.94507e-01\n",
      "   2.21211e-06]]\n",
      "Episode 770: log10(loss): -1.84113\n",
      "[[ 7.66020e-05  8.36271e+00  8.22214e-01  4.59495e+00  2.00188e-01\n",
      "   0.00000e+00 -3.95403e+00  4.59462e+00  0.00000e+00  3.92411e-01\n",
      "   2.01987e-06]]\n",
      "Episode 780: log10(loss): -1.84129\n",
      "[[ 7.15246e-05  8.41610e+00  8.22214e-01  4.59765e+00  2.00176e-01\n",
      "   0.00000e+00 -3.95648e+00  4.59734e+00  0.00000e+00  3.90233e-01\n",
      "   1.84809e-06]]\n",
      "Episode 790: log10(loss): -1.84144\n",
      "[[ 6.69077e-05  8.47015e+00  8.22214e-01  4.59998e+00  2.00164e-01\n",
      "   0.00000e+00 -3.95858e+00  4.59971e+00  0.00000e+00  3.88010e-01\n",
      "   1.69495e-06]]\n",
      "Episode 800: log10(loss): -1.84157\n",
      "[[ 6.27074e-05  8.52440e+00  8.22214e-01  4.60203e+00  2.00154e-01\n",
      "   0.00000e+00 -3.96042e+00  4.60179e+00  0.00000e+00  3.85775e-01\n",
      "   1.55829e-06]]\n",
      "Episode 810: log10(loss): -1.84169\n",
      "[[ 5.88956e-05  8.57826e+00  8.22214e-01  4.60376e+00  2.00144e-01\n",
      "   0.00000e+00 -3.96194e+00  4.60355e+00  0.00000e+00  3.83549e-01\n",
      "   1.43664e-06]]\n",
      "Episode 820: log10(loss): -1.84180\n",
      "[[ 5.54316e-05  8.63147e+00  8.22214e-01  4.60526e+00  2.00136e-01\n",
      "   0.00000e+00 -3.96321e+00  4.60506e+00  0.00000e+00  3.81353e-01\n",
      "   1.32806e-06]]\n",
      "Episode 830: log10(loss): -1.84191\n",
      "[[ 5.22830e-05  8.68370e+00  8.22214e-01  4.60651e+00  2.00128e-01\n",
      "   0.00000e+00 -3.96425e+00  4.60633e+00  0.00000e+00  3.79200e-01\n",
      "   1.23105e-06]]\n",
      "Episode 840: log10(loss): -1.84200\n",
      "[[ 4.94215e-05  8.73470e+00  8.22214e-01  4.60754e+00  2.00121e-01\n",
      "   0.00000e+00 -3.96504e+00  4.60738e+00  0.00000e+00  3.77100e-01\n",
      "   1.14438e-06]]\n",
      "Episode 850: log10(loss): -1.84208\n",
      "[[ 4.68152e-05  8.78434e+00  8.22214e-01  4.60839e+00  2.00114e-01\n",
      "   0.00000e+00 -3.96566e+00  4.60824e+00  0.00000e+00  3.75063e-01\n",
      "   1.06673e-06]]\n",
      "Episode 860: log10(loss): -1.84216\n",
      "[[ 4.44245e-05  8.83283e+00  8.22214e-01  4.60918e+00  2.00108e-01\n",
      "   0.00000e+00 -3.96625e+00  4.60905e+00  0.00000e+00  3.73092e-01\n",
      "   9.96582e-07]]\n",
      "Episode 870: log10(loss): -1.84223\n",
      "[[ 4.22012e-05  8.88068e+00  8.22214e-01  4.61015e+00  2.00103e-01\n",
      "   0.00000e+00 -3.96714e+00  4.61003e+00  0.00000e+00  3.71187e-01\n",
      "   9.32221e-07]]\n",
      "Episode 880: log10(loss): -1.84230\n",
      "[[ 4.01177e-05  8.92814e+00  8.22214e-01  4.61142e+00  2.00098e-01\n",
      "   0.00000e+00 -3.96851e+00  4.61131e+00  0.00000e+00  3.69353e-01\n",
      "   8.72709e-07]]\n",
      "Episode 890: log10(loss): -1.84236\n",
      "[[ 3.81346e-05  8.97584e+00  8.22214e-01  4.61324e+00  2.00093e-01\n",
      "   0.00000e+00 -3.97069e+00  4.61314e+00  0.00000e+00  3.67588e-01\n",
      "   8.16795e-07]]\n",
      "Episode 900: log10(loss): -1.84242\n",
      "[[ 3.62642e-05  9.02329e+00  8.22214e-01  4.61543e+00  2.00088e-01\n",
      "   0.00000e+00 -3.97342e+00  4.61534e+00  0.00000e+00  3.65892e-01\n",
      "   7.64815e-07]]\n",
      "Episode 910: log10(loss): -1.84248\n",
      "[[ 3.45696e-05  9.06850e+00  8.22214e-01  4.61729e+00  2.00084e-01\n",
      "   0.00000e+00 -3.97573e+00  4.61721e+00  0.00000e+00  3.64268e-01\n",
      "   7.18499e-07]]\n",
      "Episode 920: log10(loss): -1.84253\n",
      "[[ 3.30591e-05  9.11069e+00  8.22214e-01  4.61858e+00  2.00080e-01\n",
      "   0.00000e+00 -3.97726e+00  4.61850e+00  0.00000e+00  3.62719e-01\n",
      "   6.77894e-07]]\n",
      "Episode 930: log10(loss): -1.84257\n",
      "[[ 3.16953e-05  9.15044e+00  8.22214e-01  4.61948e+00  2.00077e-01\n",
      "   0.00000e+00 -3.97827e+00  4.61941e+00  0.00000e+00  3.61239e-01\n",
      "   6.41742e-07]]\n",
      "Episode 940: log10(loss): -1.84261\n",
      "[[ 3.04473e-05  9.18828e+00  8.22214e-01  4.62020e+00  2.00074e-01\n",
      "   0.00000e+00 -3.97905e+00  4.62013e+00  0.00000e+00  3.59827e-01\n",
      "   6.09077e-07]]\n",
      "Episode 950: log10(loss): -1.84265\n",
      "[[ 2.92974e-05  9.22444e+00  8.22214e-01  4.62080e+00  2.00071e-01\n",
      "   0.00000e+00 -3.97968e+00  4.62074e+00  0.00000e+00  3.58480e-01\n",
      "   5.79322e-07]]\n",
      "Episode 960: log10(loss): -1.84269\n",
      "[[ 2.82357e-05  9.25900e+00  8.22214e-01  4.62131e+00  2.00068e-01\n",
      "   0.00000e+00 -3.98020e+00  4.62125e+00  0.00000e+00  3.57196e-01\n",
      "   5.52131e-07]]\n",
      "Episode 970: log10(loss): -1.84272\n",
      "[[ 2.72518e-05  9.29209e+00  8.22214e-01  4.62175e+00  2.00066e-01\n",
      "   0.00000e+00 -3.98063e+00  4.62169e+00  0.00000e+00  3.55969e-01\n",
      "   5.27172e-07]]\n",
      "Episode 980: log10(loss): -1.84275\n",
      "[[ 2.63340e-05  9.32390e+00  8.22214e-01  4.62217e+00  2.00064e-01\n",
      "   0.00000e+00 -3.98106e+00  4.62211e+00  0.00000e+00  3.54799e-01\n",
      "   5.04110e-07]]\n",
      "Episode 990: log10(loss): -1.84278\n",
      "[[ 2.54785e-05  9.35441e+00  8.22214e-01  4.62256e+00  2.00062e-01\n",
      "   0.00000e+00 -3.98145e+00  4.62251e+00  0.00000e+00  3.53682e-01\n",
      "   4.82829e-07]]\n",
      "Episode 1000: log10(loss): -1.84280\n",
      "[[ 2.46816e-05  9.38360e+00  8.22214e-01  4.62290e+00  2.00060e-01\n",
      "   0.00000e+00 -3.98180e+00  4.62286e+00  0.00000e+00  3.52618e-01\n",
      "   4.63179e-07]]\n",
      "Episode 1010: log10(loss): -1.84283\n",
      "[[ 2.39369e-05  9.41155e+00  8.22214e-01  4.62322e+00  2.00058e-01\n",
      "   0.00000e+00 -3.98211e+00  4.62317e+00  0.00000e+00  3.51603e-01\n",
      "   4.44979e-07]]\n",
      "Episode 1020: log10(loss): -1.84285\n",
      "[[ 2.32399e-05  9.43831e+00  8.22214e-01  4.62350e+00  2.00056e-01\n",
      "   0.00000e+00 -3.98238e+00  4.62346e+00  0.00000e+00  3.50635e-01\n",
      "   4.28094e-07]]\n",
      "Episode 1030: log10(loss): -1.84287\n",
      "[[ 2.25819e-05  9.46413e+00  8.22214e-01  4.62383e+00  2.00054e-01\n",
      "   0.00000e+00 -3.98273e+00  4.62378e+00  0.00000e+00  3.49712e-01\n",
      "   4.12279e-07]]\n",
      "Episode 1040: log10(loss): -1.84289\n",
      "[[ 2.19692e-05  9.48867e+00  8.22214e-01  4.62405e+00  2.00053e-01\n",
      "   0.00000e+00 -3.98293e+00  4.62401e+00  0.00000e+00  3.48830e-01\n",
      "   3.97681e-07]]\n",
      "Episode 1050: log10(loss): -1.84291\n",
      "[[ 2.13897e-05  9.51231e+00  8.22214e-01  4.62431e+00  2.00052e-01\n",
      "   0.00000e+00 -3.98319e+00  4.62427e+00  0.00000e+00  3.47990e-01\n",
      "   3.83977e-07]]\n",
      "Episode 1060: log10(loss): -1.84293\n",
      "[[ 2.08446e-05  9.53493e+00  8.22214e-01  4.62454e+00  2.00050e-01\n",
      "   0.00000e+00 -3.98343e+00  4.62450e+00  0.00000e+00  3.47188e-01\n",
      "   3.71186e-07]]\n",
      "Episode 1070: log10(loss): -1.84295\n",
      "[[ 2.03312e-05  9.55659e+00  8.22214e-01  4.62475e+00  2.00049e-01\n",
      "   0.00000e+00 -3.98364e+00  4.62472e+00  0.00000e+00  3.46423e-01\n",
      "   3.59224e-07]]\n",
      "Episode 1080: log10(loss): -1.84296\n",
      "[[ 1.98475e-05  9.57729e+00  8.22214e-01  4.62494e+00  2.00048e-01\n",
      "   0.00000e+00 -3.98382e+00  4.62490e+00  0.00000e+00  3.45693e-01\n",
      "   3.48037e-07]]\n",
      "Episode 1090: log10(loss): -1.84298\n",
      "[[ 1.93896e-05  9.59715e+00  8.22214e-01  4.62513e+00  2.00047e-01\n",
      "   0.00000e+00 -3.98401e+00  4.62509e+00  0.00000e+00  3.44997e-01\n",
      "   3.37514e-07]]\n",
      "Episode 1100: log10(loss): -1.84299\n",
      "[[ 1.89564e-05  9.61620e+00  8.22214e-01  4.62529e+00  2.00046e-01\n",
      "   0.00000e+00 -3.98417e+00  4.62526e+00  0.00000e+00  3.44330e-01\n",
      "   3.27616e-07]]\n",
      "Episode 1110: log10(loss): -1.84300\n",
      "[[ 1.85457e-05  9.63444e+00  8.22214e-01  4.62546e+00  2.00045e-01\n",
      "   0.00000e+00 -3.98434e+00  4.62543e+00  0.00000e+00  3.43695e-01\n",
      "   3.18295e-07]]\n",
      "Episode 1120: log10(loss): -1.84302\n",
      "[[ 1.81541e-05  9.65205e+00  8.22214e-01  4.62566e+00  2.00044e-01\n",
      "   0.00000e+00 -3.98455e+00  4.62563e+00  0.00000e+00  3.43088e-01\n",
      "   3.09453e-07]]\n",
      "Episode 1130: log10(loss): -1.84303\n",
      "[[ 1.77864e-05  9.66872e+00  8.22214e-01  4.62576e+00  2.00043e-01\n",
      "   0.00000e+00 -3.98464e+00  4.62573e+00  0.00000e+00  3.42507e-01\n",
      "   3.01211e-07]]\n",
      "Episode 1140: log10(loss): -1.84304\n",
      "[[ 1.74331e-05  9.68490e+00  8.22214e-01  4.62592e+00  2.00042e-01\n",
      "   0.00000e+00 -3.98481e+00  4.62589e+00  0.00000e+00  3.41951e-01\n",
      "   2.93332e-07]]\n",
      "Episode 1150: log10(loss): -1.84305\n",
      "[[ 1.71007e-05  9.70021e+00  8.22214e-01  4.62602e+00  2.00041e-01\n",
      "   0.00000e+00 -3.98489e+00  4.62599e+00  0.00000e+00  3.41420e-01\n",
      "   2.85961e-07]]\n",
      "Episode 1160: log10(loss): -1.84306\n",
      "[[ 1.67812e-05  9.71505e+00  8.22214e-01  4.62614e+00  2.00040e-01\n",
      "   0.00000e+00 -3.98501e+00  4.62612e+00  0.00000e+00  3.40911e-01\n",
      "   2.78916e-07]]\n",
      "Episode 1170: log10(loss): -1.84307\n",
      "[[ 1.64777e-05  9.72924e+00  8.22214e-01  4.62625e+00  2.00039e-01\n",
      "   0.00000e+00 -3.98512e+00  4.62623e+00  0.00000e+00  3.40425e-01\n",
      "   2.72259e-07]]\n",
      "Episode 1180: log10(loss): -1.84308\n",
      "[[ 1.61886e-05  9.74281e+00  8.22214e-01  4.62634e+00  2.00039e-01\n",
      "   0.00000e+00 -3.98520e+00  4.62631e+00  0.00000e+00  3.39960e-01\n",
      "   2.65953e-07]]\n",
      "Episode 1190: log10(loss): -1.84309\n",
      "[[ 1.59135e-05  9.75576e+00  8.22214e-01  4.62640e+00  2.00038e-01\n",
      "   0.00000e+00 -3.98523e+00  4.62637e+00  0.00000e+00  3.39514e-01\n",
      "   2.59987e-07]]\n",
      "Episode 1200: log10(loss): -1.84310\n",
      "[[ 1.56485e-05  9.76830e+00  8.22214e-01  4.62648e+00  2.00037e-01\n",
      "   0.00000e+00 -3.98531e+00  4.62646e+00  0.00000e+00  3.39087e-01\n",
      "   2.54270e-07]]\n",
      "Episode 1210: log10(loss): -1.84311\n",
      "[[ 1.53960e-05  9.78027e+00  8.22214e-01  4.62654e+00  2.00037e-01\n",
      "   0.00000e+00 -3.98536e+00  4.62652e+00  0.00000e+00  3.38678e-01\n",
      "   2.48848e-07]]\n",
      "Episode 1220: log10(loss): -1.84311\n",
      "[[ 1.51504e-05  9.79199e+00  8.22214e-01  4.62667e+00  2.00036e-01\n",
      "   0.00000e+00 -3.98550e+00  4.62666e+00  0.00000e+00  3.38285e-01\n",
      "   2.43598e-07]]\n",
      "Episode 1230: log10(loss): -1.84312\n",
      "[[ 1.49181e-05  9.80303e+00  8.22214e-01  4.62673e+00  2.00036e-01\n",
      "   0.00000e+00 -3.98555e+00  4.62671e+00  0.00000e+00  3.37910e-01\n",
      "   2.38659e-07]]\n",
      "Episode 1240: log10(loss): -1.84313\n",
      "[[ 1.46951e-05  9.81367e+00  8.22214e-01  4.62678e+00  2.00035e-01\n",
      "   0.00000e+00 -3.98559e+00  4.62677e+00  0.00000e+00  3.37549e-01\n",
      "   2.33939e-07]]\n",
      "Episode 1250: log10(loss): -1.84313\n",
      "[[ 1.44813e-05  9.82385e+00  8.22214e-01  4.62683e+00  2.00035e-01\n",
      "   0.00000e+00 -3.98563e+00  4.62681e+00  0.00000e+00  3.37204e-01\n",
      "   2.29430e-07]]\n",
      "Episode 1260: log10(loss): -1.84314\n",
      "[[ 1.42717e-05  9.83394e+00  8.22214e-01  4.62695e+00  2.00034e-01\n",
      "   0.00000e+00 -3.98576e+00  4.62693e+00  0.00000e+00  3.36870e-01\n",
      "   2.25026e-07]]\n",
      "Episode 1270: log10(loss): -1.84315\n",
      "[[ 1.40710e-05  9.84357e+00  8.22214e-01  4.62704e+00  2.00034e-01\n",
      "   0.00000e+00 -3.98586e+00  4.62703e+00  0.00000e+00  3.36550e-01\n",
      "   2.20831e-07]]\n",
      "Episode 1280: log10(loss): -1.84315\n",
      "[[ 1.38772e-05  9.85287e+00  8.22214e-01  4.62715e+00  2.00033e-01\n",
      "   0.00000e+00 -3.98598e+00  4.62714e+00  0.00000e+00  3.36243e-01\n",
      "   2.16796e-07]]\n",
      "Episode 1290: log10(loss): -1.84316\n",
      "[[ 1.36900e-05  9.86186e+00  8.22214e-01  4.62726e+00  2.00033e-01\n",
      "   0.00000e+00 -3.98610e+00  4.62724e+00  0.00000e+00  3.35946e-01\n",
      "   2.12919e-07]]\n",
      "Episode 1300: log10(loss): -1.84316\n",
      "[[ 1.35065e-05  9.87074e+00  8.22214e-01  4.62742e+00  2.00032e-01\n",
      "   0.00000e+00 -3.98630e+00  4.62740e+00  0.00000e+00  3.35660e-01\n",
      "   2.09129e-07]]\n",
      "Episode 1310: log10(loss): -1.84317\n",
      "[[ 1.33306e-05  9.87922e+00  8.22214e-01  4.62755e+00  2.00032e-01\n",
      "   0.00000e+00 -3.98646e+00  4.62753e+00  0.00000e+00  3.35385e-01\n",
      "   2.05511e-07]]\n",
      "Episode 1320: log10(loss): -1.84318\n",
      "[[ 1.31611e-05  9.88736e+00  8.22214e-01  4.62767e+00  2.00031e-01\n",
      "   0.00000e+00 -3.98660e+00  4.62766e+00  0.00000e+00  3.35120e-01\n",
      "   2.02041e-07]]\n",
      "Episode 1330: log10(loss): -1.84318\n",
      "[[ 1.29975e-05  9.89522e+00  8.22214e-01  4.62779e+00  2.00031e-01\n",
      "   0.00000e+00 -3.98675e+00  4.62777e+00  0.00000e+00  3.34866e-01\n",
      "   1.98704e-07]]\n",
      "Episode 1340: log10(loss): -1.84319\n",
      "[[ 1.28396e-05  9.90276e+00  8.22214e-01  4.62789e+00  2.00031e-01\n",
      "   0.00000e+00 -3.98687e+00  4.62787e+00  0.00000e+00  3.34621e-01\n",
      "   1.95496e-07]]\n",
      "Episode 1350: log10(loss): -1.84319\n",
      "[[ 1.26856e-05  9.91016e+00  8.22214e-01  4.62801e+00  2.00030e-01\n",
      "   0.00000e+00 -3.98702e+00  4.62800e+00  0.00000e+00  3.34383e-01\n",
      "   1.92376e-07]]\n",
      "Episode 1360: log10(loss): -1.84319\n",
      "[[ 1.25383e-05  9.91715e+00  8.22214e-01  4.62810e+00  2.00030e-01\n",
      "   0.00000e+00 -3.98712e+00  4.62808e+00  0.00000e+00  3.34156e-01\n",
      "   1.89405e-07]]\n",
      "Episode 1370: log10(loss): -1.84320\n",
      "[[ 1.23960e-05  9.92387e+00  8.22214e-01  4.62817e+00  2.00030e-01\n",
      "   0.00000e+00 -3.98720e+00  4.62815e+00  0.00000e+00  3.33937e-01\n",
      "   1.86543e-07]]\n",
      "Episode 1380: log10(loss): -1.84320\n",
      "[[ 1.22563e-05  9.93049e+00  8.22214e-01  4.62828e+00  2.00029e-01\n",
      "   0.00000e+00 -3.98734e+00  4.62827e+00  0.00000e+00  3.33725e-01\n",
      "   1.83745e-07]]\n",
      "Episode 1390: log10(loss): -1.84321\n",
      "[[ 1.21232e-05  9.93672e+00  8.22214e-01  4.62832e+00  2.00029e-01\n",
      "   0.00000e+00 -3.98738e+00  4.62831e+00  0.00000e+00  3.33520e-01\n",
      "   1.81088e-07]]\n",
      "Episode 1400: log10(loss): -1.84321\n",
      "[[ 1.19934e-05  9.94278e+00  8.22214e-01  4.62838e+00  2.00029e-01\n",
      "   0.00000e+00 -3.98744e+00  4.62837e+00  0.00000e+00  3.33322e-01\n",
      "   1.78505e-07]]\n",
      "Episode 1410: log10(loss): -1.84322\n",
      "[[ 1.18651e-05  9.94883e+00  8.22214e-01  4.62849e+00  2.00028e-01\n",
      "   0.00000e+00 -3.98758e+00  4.62848e+00  0.00000e+00  3.33130e-01\n",
      "   1.75957e-07]]\n",
      "Episode 1420: log10(loss): -1.84322\n",
      "[[ 1.17416e-05  9.95458e+00  8.22214e-01  4.62857e+00  2.00028e-01\n",
      "   0.00000e+00 -3.98767e+00  4.62855e+00  0.00000e+00  3.32946e-01\n",
      "   1.73515e-07]]\n",
      "Episode 1430: log10(loss): -1.84322\n",
      "[[ 1.16225e-05  9.96008e+00  8.22214e-01  4.62860e+00  2.00028e-01\n",
      "   0.00000e+00 -3.98770e+00  4.62859e+00  0.00000e+00  3.32765e-01\n",
      "   1.71170e-07]]\n",
      "Episode 1440: log10(loss): -1.84323\n",
      "[[ 1.15053e-05  9.96551e+00  8.22214e-01  4.62868e+00  2.00027e-01\n",
      "   0.00000e+00 -3.98780e+00  4.62867e+00  0.00000e+00  3.32592e-01\n",
      "   1.68867e-07]]\n",
      "Episode 1450: log10(loss): -1.84323\n",
      "[[ 1.13932e-05  9.97061e+00  8.22214e-01  4.62871e+00  2.00027e-01\n",
      "   0.00000e+00 -3.98782e+00  4.62869e+00  0.00000e+00  3.32424e-01\n",
      "   1.66672e-07]]\n",
      "Episode 1460: log10(loss): -1.84323\n",
      "[[ 1.12824e-05  9.97568e+00  8.22214e-01  4.62877e+00  2.00027e-01\n",
      "   0.00000e+00 -3.98790e+00  4.62876e+00  0.00000e+00  3.32261e-01\n",
      "   1.64509e-07]]\n",
      "Episode 1470: log10(loss): -1.84324\n",
      "[[ 1.11764e-05  9.98045e+00  8.22214e-01  4.62880e+00  2.00027e-01\n",
      "   0.00000e+00 -3.98791e+00  4.62878e+00  0.00000e+00  3.32105e-01\n",
      "   1.62447e-07]]\n",
      "Episode 1480: log10(loss): -1.84324\n",
      "[[ 1.10708e-05  9.98526e+00  8.22214e-01  4.62887e+00  2.00026e-01\n",
      "   0.00000e+00 -3.98801e+00  4.62886e+00  0.00000e+00  3.31952e-01\n",
      "   1.60399e-07]]\n",
      "Episode 1490: log10(loss): -1.84324\n",
      "[[ 1.09687e-05  9.98986e+00  8.22214e-01  4.62893e+00  2.00026e-01\n",
      "   0.00000e+00 -3.98807e+00  4.62891e+00  0.00000e+00  3.31805e-01\n",
      "   1.58426e-07]]\n",
      "Episode 1500: log10(loss): -1.84325\n",
      "[[ 1.08697e-05  9.99427e+00  8.22214e-01  4.62894e+00  2.00026e-01\n",
      "   0.00000e+00 -3.98808e+00  4.62893e+00  0.00000e+00  3.31660e-01\n",
      "   1.56520e-07]]\n",
      "Episode 1510: log10(loss): -1.84325\n",
      "[[ 1.07736e-05  9.99850e+00  8.22214e-01  4.62896e+00  2.00026e-01\n",
      "   0.00000e+00 -3.98809e+00  4.62894e+00  0.00000e+00  3.31521e-01\n",
      "   1.54677e-07]]\n",
      "Episode 1520: log10(loss): -1.84325\n",
      "[[ 1.06782e-05  1.00027e+01  8.22214e-01  4.62901e+00  2.00025e-01\n",
      "   0.00000e+00 -3.98815e+00  4.62900e+00  0.00000e+00  3.31385e-01\n",
      "   1.52852e-07]]\n",
      "Episode 1530: log10(loss): -1.84325\n",
      "[[ 1.05856e-05  1.00068e+01  8.22214e-01  4.62903e+00  2.00025e-01\n",
      "   0.00000e+00 -3.98817e+00  4.62902e+00  0.00000e+00  3.31253e-01\n",
      "   1.51088e-07]]\n",
      "Episode 1540: log10(loss): -1.84326\n",
      "[[ 1.04939e-05  1.00109e+01  8.22214e-01  4.62910e+00  2.00025e-01\n",
      "   0.00000e+00 -3.98825e+00  4.62910e+00  0.00000e+00  3.31125e-01\n",
      "   1.49344e-07]]\n",
      "Episode 1550: log10(loss): -1.84326\n",
      "[[ 1.04050e-05  1.00148e+01  8.22214e-01  4.62915e+00  2.00025e-01\n",
      "   0.00000e+00 -3.98831e+00  4.62914e+00  0.00000e+00  3.31001e-01\n",
      "   1.47661e-07]]\n",
      "Episode 1560: log10(loss): -1.84326\n",
      "[[ 1.03184e-05  1.00185e+01  8.22214e-01  4.62918e+00  2.00025e-01\n",
      "   0.00000e+00 -3.98833e+00  4.62916e+00  0.00000e+00  3.30880e-01\n",
      "   1.46027e-07]]\n",
      "Episode 1570: log10(loss): -1.84326\n",
      "[[ 1.02328e-05  1.00223e+01  8.22214e-01  4.62922e+00  2.00024e-01\n",
      "   0.00000e+00 -3.98839e+00  4.62922e+00  0.00000e+00  3.30760e-01\n",
      "   1.44414e-07]]\n",
      "Episode 1580: log10(loss): -1.84327\n",
      "[[ 1.01497e-05  1.00259e+01  8.22214e-01  4.62925e+00  2.00024e-01\n",
      "   0.00000e+00 -3.98842e+00  4.62924e+00  0.00000e+00  3.30645e-01\n",
      "   1.42856e-07]]\n",
      "Episode 1590: log10(loss): -1.84327\n",
      "[[ 1.00681e-05  1.00294e+01  8.22214e-01  4.62929e+00  2.00024e-01\n",
      "   0.00000e+00 -3.98846e+00  4.62928e+00  0.00000e+00  3.30533e-01\n",
      "   1.41329e-07]]\n",
      "Episode 1600: log10(loss): -1.84327\n",
      "[[ 9.98802e-06  1.00328e+01  8.22214e-01  4.62933e+00  2.00024e-01\n",
      "   0.00000e+00 -3.98850e+00  4.62931e+00  0.00000e+00  3.30422e-01\n",
      "   1.39833e-07]]\n",
      "Episode 1610: log10(loss): -1.84327\n",
      "[[ 9.91003e-06  1.00362e+01  8.22214e-01  4.62935e+00  2.00024e-01\n",
      "   0.00000e+00 -3.98853e+00  4.62934e+00  0.00000e+00  3.30315e-01\n",
      "   1.38381e-07]]\n",
      "Episode 1620: log10(loss): -1.84328\n",
      "[[ 9.83397e-06  1.00394e+01  8.22214e-01  4.62937e+00  2.00023e-01\n",
      "   0.00000e+00 -3.98854e+00  4.62936e+00  0.00000e+00  3.30211e-01\n",
      "   1.36966e-07]]\n",
      "Episode 1630: log10(loss): -1.84328\n",
      "[[ 9.75964e-06  1.00425e+01  8.22214e-01  4.62938e+00  2.00023e-01\n",
      "   0.00000e+00 -3.98855e+00  4.62938e+00  0.00000e+00  3.30110e-01\n",
      "   1.35586e-07]]\n",
      "Episode 1640: log10(loss): -1.84328\n",
      "[[ 9.68551e-06  1.00457e+01  8.22214e-01  4.62944e+00  2.00023e-01\n",
      "   0.00000e+00 -3.98862e+00  4.62943e+00  0.00000e+00  3.30011e-01\n",
      "   1.34214e-07]]\n",
      "Episode 1650: log10(loss): -1.84328\n",
      "[[ 9.61364e-06  1.00487e+01  8.22214e-01  4.62946e+00  2.00023e-01\n",
      "   0.00000e+00 -3.98864e+00  4.62945e+00  0.00000e+00  3.29915e-01\n",
      "   1.32887e-07]]\n",
      "Episode 1660: log10(loss): -1.84328\n",
      "[[ 9.54282e-06  1.00516e+01  8.22214e-01  4.62950e+00  2.00023e-01\n",
      "   0.00000e+00 -3.98869e+00  4.62949e+00  0.00000e+00  3.29822e-01\n",
      "   1.31582e-07]]\n",
      "Episode 1670: log10(loss): -1.84329\n",
      "[[ 9.47404e-06  1.00544e+01  8.22214e-01  4.62951e+00  2.00022e-01\n",
      "   0.00000e+00 -3.98869e+00  4.62950e+00  0.00000e+00  3.29730e-01\n",
      "   1.30319e-07]]\n",
      "Episode 1680: log10(loss): -1.84329\n",
      "[[ 9.40629e-06  1.00572e+01  8.22214e-01  4.62952e+00  2.00022e-01\n",
      "   0.00000e+00 -3.98870e+00  4.62951e+00  0.00000e+00  3.29640e-01\n",
      "   1.29079e-07]]\n",
      "Episode 1690: log10(loss): -1.84329\n",
      "[[ 9.33947e-06  1.00600e+01  8.22214e-01  4.62954e+00  2.00022e-01\n",
      "   0.00000e+00 -3.98872e+00  4.62953e+00  0.00000e+00  3.29552e-01\n",
      "   1.27861e-07]]\n",
      "Episode 1700: log10(loss): -1.84329\n",
      "[[ 9.27358e-06  1.00627e+01  8.22214e-01  4.62957e+00  2.00022e-01\n",
      "   0.00000e+00 -3.98875e+00  4.62955e+00  0.00000e+00  3.29465e-01\n",
      "   1.26661e-07]]\n",
      "Episode 1710: log10(loss): -1.84329\n",
      "[[ 9.20893e-06  1.00653e+01  8.22214e-01  4.62958e+00  2.00022e-01\n",
      "   0.00000e+00 -3.98876e+00  4.62957e+00  0.00000e+00  3.29379e-01\n",
      "   1.25488e-07]]\n",
      "Episode 1720: log10(loss): -1.84330\n",
      "[[ 9.14514e-06  1.00680e+01  8.22214e-01  4.62961e+00  2.00022e-01\n",
      "   0.00000e+00 -3.98880e+00  4.62960e+00  0.00000e+00  3.29297e-01\n",
      "   1.24332e-07]]\n",
      "Episode 1730: log10(loss): -1.84330\n",
      "[[ 9.08187e-06  1.00706e+01  8.22214e-01  4.62966e+00  2.00022e-01\n",
      "   0.00000e+00 -3.98886e+00  4.62965e+00  0.00000e+00  3.29216e-01\n",
      "   1.23189e-07]]\n",
      "Episode 1740: log10(loss): -1.84330\n",
      "[[ 9.02098e-06  1.00730e+01  8.22214e-01  4.62966e+00  2.00021e-01\n",
      "   0.00000e+00 -3.98885e+00  4.62966e+00  0.00000e+00  3.29136e-01\n",
      "   1.22091e-07]]\n",
      "Episode 1750: log10(loss): -1.84330\n",
      "[[ 8.96052e-06  1.00755e+01  8.22214e-01  4.62969e+00  2.00021e-01\n",
      "   0.00000e+00 -3.98888e+00  4.62968e+00  0.00000e+00  3.29058e-01\n",
      "   1.21006e-07]]\n",
      "Episode 1760: log10(loss): -1.84330\n",
      "[[ 8.90066e-06  1.00779e+01  8.22214e-01  4.62972e+00  2.00021e-01\n",
      "   0.00000e+00 -3.98892e+00  4.62971e+00  0.00000e+00  3.28982e-01\n",
      "   1.19930e-07]]\n",
      "Episode 1770: log10(loss): -1.84331\n",
      "[[ 8.84218e-06  1.00803e+01  8.22214e-01  4.62974e+00  2.00021e-01\n",
      "   0.00000e+00 -3.98894e+00  4.62973e+00  0.00000e+00  3.28907e-01\n",
      "   1.18883e-07]]\n",
      "Episode 1780: log10(loss): -1.84331\n",
      "[[ 8.78448e-06  1.00826e+01  8.22214e-01  4.62977e+00  2.00021e-01\n",
      "   0.00000e+00 -3.98897e+00  4.62976e+00  0.00000e+00  3.28833e-01\n",
      "   1.17852e-07]]\n",
      "Episode 1790: log10(loss): -1.84331\n",
      "[[ 8.72778e-06  1.00849e+01  8.22214e-01  4.62978e+00  2.00021e-01\n",
      "   0.00000e+00 -3.98899e+00  4.62978e+00  0.00000e+00  3.28761e-01\n",
      "   1.16841e-07]]\n",
      "Episode 1800: log10(loss): -1.84331\n",
      "[[ 8.67211e-06  1.00871e+01  8.22214e-01  4.62980e+00  2.00021e-01\n",
      "   0.00000e+00 -3.98900e+00  4.62979e+00  0.00000e+00  3.28690e-01\n",
      "   1.15852e-07]]\n",
      "Episode 1810: log10(loss): -1.84331\n",
      "[[ 8.61752e-06  1.00892e+01  8.22214e-01  4.62980e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98900e+00  4.62980e+00  0.00000e+00  3.28621e-01\n",
      "   1.14884e-07]]\n",
      "Episode 1820: log10(loss): -1.84331\n",
      "[[ 8.56276e-06  1.00915e+01  8.22214e-01  4.62984e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98904e+00  4.62983e+00  0.00000e+00  3.28552e-01\n",
      "   1.13916e-07]]\n",
      "Episode 1830: log10(loss): -1.84331\n",
      "[[ 8.50920e-06  1.00936e+01  8.22214e-01  4.62985e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98906e+00  4.62985e+00  0.00000e+00  3.28483e-01\n",
      "   1.12971e-07]]\n",
      "Episode 1840: log10(loss): -1.84332\n",
      "[[ 8.45677e-06  1.00957e+01  8.22214e-01  4.62988e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98908e+00  4.62987e+00  0.00000e+00  3.28418e-01\n",
      "   1.12047e-07]]\n",
      "Episode 1850: log10(loss): -1.84332\n",
      "[[ 8.40496e-06  1.00977e+01  8.22214e-01  4.62990e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98911e+00  4.62989e+00  0.00000e+00  3.28353e-01\n",
      "   1.11138e-07]]\n",
      "Episode 1860: log10(loss): -1.84332\n",
      "[[ 8.35380e-06  1.00998e+01  8.22214e-01  4.62991e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98912e+00  4.62990e+00  0.00000e+00  3.28288e-01\n",
      "   1.10241e-07]]\n",
      "Episode 1870: log10(loss): -1.84332\n",
      "[[ 8.30454e-06  1.01017e+01  8.22214e-01  4.62989e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98909e+00  4.62988e+00  0.00000e+00  3.28225e-01\n",
      "   1.09381e-07]]\n",
      "Episode 1880: log10(loss): -1.84332\n",
      "[[ 8.25443e-06  1.01037e+01  8.22214e-01  4.62994e+00  2.00020e-01\n",
      "   0.00000e+00 -3.98915e+00  4.62993e+00  0.00000e+00  3.28164e-01\n",
      "   1.08504e-07]]\n",
      "Episode 1890: log10(loss): -1.84332\n",
      "[[ 8.20619e-06  1.01055e+01  8.22214e-01  4.62994e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98915e+00  4.62993e+00  0.00000e+00  3.28105e-01\n",
      "   1.07662e-07]]\n",
      "Episode 1900: log10(loss): -1.84333\n",
      "[[ 8.15797e-06  1.01074e+01  8.22214e-01  4.62996e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98917e+00  4.62996e+00  0.00000e+00  3.28045e-01\n",
      "   1.06824e-07]]\n",
      "Episode 1910: log10(loss): -1.84333\n",
      "[[ 8.11128e-06  1.01092e+01  8.22214e-01  4.62996e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98917e+00  4.62995e+00  0.00000e+00  3.27987e-01\n",
      "   1.06014e-07]]\n",
      "Episode 1920: log10(loss): -1.84333\n",
      "[[ 8.06447e-06  1.01110e+01  8.22214e-01  4.62998e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98919e+00  4.62997e+00  0.00000e+00  3.27930e-01\n",
      "   1.05204e-07]]\n",
      "Episode 1930: log10(loss): -1.84333\n",
      "[[ 8.01817e-06  1.01128e+01  8.22214e-01  4.63001e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98922e+00  4.63000e+00  0.00000e+00  3.27873e-01\n",
      "   1.04403e-07]]\n",
      "Episode 1940: log10(loss): -1.84333\n",
      "[[ 7.97317e-06  1.01145e+01  8.22214e-01  4.63001e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98921e+00  4.63000e+00  0.00000e+00  3.27818e-01\n",
      "   1.03628e-07]]\n",
      "Episode 1950: log10(loss): -1.84333\n",
      "[[ 7.92806e-06  1.01163e+01  8.22214e-01  4.63003e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98924e+00  4.63002e+00  0.00000e+00  3.27763e-01\n",
      "   1.02853e-07]]\n",
      "Episode 1960: log10(loss): -1.84333\n",
      "[[ 7.88331e-06  1.01181e+01  8.22214e-01  4.63006e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98928e+00  4.63005e+00  0.00000e+00  3.27709e-01\n",
      "   1.02085e-07]]\n",
      "Episode 1970: log10(loss): -1.84333\n",
      "[[ 7.83982e-06  1.01197e+01  8.22214e-01  4.63006e+00  2.00019e-01\n",
      "   0.00000e+00 -3.98928e+00  4.63006e+00  0.00000e+00  3.27656e-01\n",
      "   1.01341e-07]]\n",
      "Episode 1980: log10(loss): -1.84334\n",
      "[[ 7.79684e-06  1.01214e+01  8.22214e-01  4.63007e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98928e+00  4.63006e+00  0.00000e+00  3.27603e-01\n",
      "   1.00608e-07]]\n",
      "Episode 1990: log10(loss): -1.84334\n",
      "[[ 7.75407e-06  1.01230e+01  8.22214e-01  4.63009e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98931e+00  4.63008e+00  0.00000e+00  3.27552e-01\n",
      "   9.98803e-08]]\n",
      "Episode 2000: log10(loss): -1.84334\n",
      "[[ 7.71219e-06  1.01246e+01  8.22214e-01  4.63009e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98931e+00  4.63009e+00  0.00000e+00  3.27501e-01\n",
      "   9.91687e-08]]\n",
      "Episode 2010: log10(loss): -1.84334\n",
      "[[ 7.67057e-06  1.01262e+01  8.22214e-01  4.63011e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98932e+00  4.63011e+00  0.00000e+00  3.27451e-01\n",
      "   9.84634e-08]]\n",
      "Episode 2020: log10(loss): -1.84334\n",
      "[[ 7.62919e-06  1.01278e+01  8.22214e-01  4.63014e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98936e+00  4.63013e+00  0.00000e+00  3.27402e-01\n",
      "   9.77644e-08]]\n",
      "Episode 2030: log10(loss): -1.84334\n",
      "[[ 7.58771e-06  1.01294e+01  8.22214e-01  4.63018e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98942e+00  4.63017e+00  0.00000e+00  3.27354e-01\n",
      "   9.70645e-08]]\n",
      "Episode 2040: log10(loss): -1.84334\n",
      "[[ 7.54843e-06  1.01308e+01  8.22214e-01  4.63016e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98938e+00  4.63015e+00  0.00000e+00  3.27306e-01\n",
      "   9.64053e-08]]\n",
      "Episode 2050: log10(loss): -1.84334\n",
      "[[ 7.50818e-06  1.01324e+01  8.22214e-01  4.63020e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98943e+00  4.63018e+00  0.00000e+00  3.27259e-01\n",
      "   9.57304e-08]]\n",
      "Episode 2060: log10(loss): -1.84334\n",
      "[[ 7.46862e-06  1.01339e+01  8.22214e-01  4.63022e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98946e+00  4.63022e+00  0.00000e+00  3.27213e-01\n",
      "   9.50677e-08]]\n",
      "Episode 2070: log10(loss): -1.84335\n",
      "[[ 7.43065e-06  1.01352e+01  8.22214e-01  4.63020e+00  2.00018e-01\n",
      "   0.00000e+00 -3.98943e+00  4.63020e+00  0.00000e+00  3.27168e-01\n",
      "   9.44350e-08]]\n",
      "Episode 2080: log10(loss): -1.84335\n",
      "[[ 7.39130e-06  1.01368e+01  8.22214e-01  4.63025e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98950e+00  4.63025e+00  0.00000e+00  3.27122e-01\n",
      "   9.37782e-08]]\n",
      "Episode 2090: log10(loss): -1.84335\n",
      "[[ 7.35354e-06  1.01382e+01  8.22214e-01  4.63027e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98952e+00  4.63027e+00  0.00000e+00  3.27079e-01\n",
      "   9.31511e-08]]\n",
      "Episode 2100: log10(loss): -1.84335\n",
      "[[ 7.31497e-06  1.01397e+01  8.22214e-01  4.63033e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98959e+00  4.63032e+00  0.00000e+00  3.27035e-01\n",
      "   9.25100e-08]]\n",
      "Episode 2110: log10(loss): -1.84335\n",
      "[[ 7.27767e-06  1.01411e+01  8.22214e-01  4.63035e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98962e+00  4.63034e+00  0.00000e+00  3.26991e-01\n",
      "   9.18922e-08]]\n",
      "Episode 2120: log10(loss): -1.84335\n",
      "[[ 7.24260e-06  1.01423e+01  8.22214e-01  4.63031e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98956e+00  4.63030e+00  0.00000e+00  3.26949e-01\n",
      "   9.13139e-08]]\n",
      "Episode 2130: log10(loss): -1.84335\n",
      "[[ 7.20549e-06  1.01438e+01  8.22214e-01  4.63036e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98963e+00  4.63036e+00  0.00000e+00  3.26908e-01\n",
      "   9.06996e-08]]\n",
      "Episode 2140: log10(loss): -1.84335\n",
      "[[ 7.17097e-06  1.01449e+01  8.22214e-01  4.63033e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98958e+00  4.63033e+00  0.00000e+00  3.26867e-01\n",
      "   9.01329e-08]]\n",
      "Episode 2150: log10(loss): -1.84335\n",
      "[[ 7.13482e-06  1.01464e+01  8.22214e-01  4.63038e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98966e+00  4.63038e+00  0.00000e+00  3.26827e-01\n",
      "   8.95371e-08]]\n",
      "Episode 2160: log10(loss): -1.84335\n",
      "[[ 7.09845e-06  1.01478e+01  8.22214e-01  4.63046e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98976e+00  4.63046e+00  0.00000e+00  3.26787e-01\n",
      "   8.89383e-08]]\n",
      "Episode 2170: log10(loss): -1.84336\n",
      "[[ 7.06426e-06  1.01491e+01  8.22214e-01  4.63047e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98977e+00  4.63046e+00  0.00000e+00  3.26748e-01\n",
      "   8.83784e-08]]\n",
      "Episode 2180: log10(loss): -1.84336\n",
      "[[ 7.03189e-06  1.01501e+01  8.22214e-01  4.63040e+00  2.00017e-01\n",
      "   0.00000e+00 -3.98967e+00  4.63040e+00  0.00000e+00  3.26708e-01\n",
      "   8.78509e-08]]\n",
      "Episode 2190: log10(loss): -1.84336\n",
      "[[ 6.99777e-06  1.01514e+01  8.22214e-01  4.63044e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98972e+00  4.63043e+00  0.00000e+00  3.26670e-01\n",
      "   8.72941e-08]]\n",
      "Episode 2200: log10(loss): -1.84336\n",
      "[[ 6.96469e-06  1.01525e+01  8.22214e-01  4.63044e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98971e+00  4.63043e+00  0.00000e+00  3.26633e-01\n",
      "   8.67565e-08]]\n",
      "Episode 2210: log10(loss): -1.84336\n",
      "[[ 6.93216e-06  1.01537e+01  8.22214e-01  4.63043e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98970e+00  4.63042e+00  0.00000e+00  3.26595e-01\n",
      "   8.62289e-08]]\n",
      "Episode 2220: log10(loss): -1.84336\n",
      "[[ 6.89815e-06  1.01551e+01  8.22214e-01  4.63049e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98977e+00  4.63048e+00  0.00000e+00  3.26556e-01\n",
      "   8.56766e-08]]\n",
      "Episode 2230: log10(loss): -1.84336\n",
      "[[ 6.86308e-06  1.01566e+01  8.22214e-01  4.63062e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98996e+00  4.63061e+00  0.00000e+00  3.26519e-01\n",
      "   8.51058e-08]]\n",
      "Episode 2240: log10(loss): -1.84336\n",
      "[[ 6.83175e-06  1.01577e+01  8.22214e-01  4.63060e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98993e+00  4.63060e+00  0.00000e+00  3.26482e-01\n",
      "   8.45985e-08]]\n",
      "Episode 2250: log10(loss): -1.84336\n",
      "[[ 6.80020e-06  1.01589e+01  8.22214e-01  4.63062e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98995e+00  4.63061e+00  0.00000e+00  3.26447e-01\n",
      "   8.40870e-08]]\n",
      "Episode 2260: log10(loss): -1.84336\n",
      "[[ 6.76672e-06  1.01603e+01  8.22214e-01  4.63073e+00  2.00016e-01\n",
      "   0.00000e+00 -3.99011e+00  4.63072e+00  0.00000e+00  3.26410e-01\n",
      "   8.35420e-08]]\n",
      "Episode 2270: log10(loss): -1.84336\n",
      "[[ 6.74209e-06  1.01606e+01  8.22214e-01  4.63047e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98973e+00  4.63046e+00  0.00000e+00  3.26376e-01\n",
      "   8.31492e-08]]\n",
      "Episode 2280: log10(loss): -1.84336\n",
      "[[ 6.72778e-06  1.01595e+01  8.22214e-01  4.62977e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98875e+00  4.62977e+00  0.00000e+00  3.26344e-01\n",
      "   8.29331e-08]]\n",
      "Episode 2290: log10(loss): -1.84337\n",
      "[[ 6.70658e-06  1.01592e+01  8.22214e-01  4.62945e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98829e+00  4.62946e+00  0.00000e+00  3.26325e-01\n",
      "   8.26000e-08]]\n",
      "Episode 2300: log10(loss): -1.84337\n",
      "[[ 6.64945e-06  1.01638e+01  8.22214e-01  4.63070e+00  2.00016e-01\n",
      "   0.00000e+00 -3.99006e+00  4.63070e+00  0.00000e+00  3.26297e-01\n",
      "   8.16567e-08]]\n",
      "Episode 2310: log10(loss): -1.84337\n",
      "[[ 6.59365e-06  1.01687e+01  8.22214e-01  4.63180e+00  2.00016e-01\n",
      "   0.00000e+00 -3.99160e+00  4.63179e+00  0.00000e+00  3.26244e-01\n",
      "   8.07367e-08]]\n",
      "Episode 2320: log10(loss): -1.84337\n",
      "[[ 6.60008e-06  1.01650e+01  8.22214e-01  4.63018e+00  2.00016e-01\n",
      "   0.00000e+00 -3.98932e+00  4.63017e+00  0.00000e+00  3.26209e-01\n",
      "   8.08729e-08]]\n",
      "Episode 2330: log10(loss): -1.84337\n",
      "[[ 6.57433e-06  1.01653e+01  8.22214e-01  4.63014e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98925e+00  4.63013e+00  0.00000e+00  3.26195e-01\n",
      "   8.04650e-08]]\n",
      "Episode 2340: log10(loss): -1.84337\n",
      "[[ 6.52882e-06  1.01687e+01  8.22214e-01  4.63086e+00  2.00015e-01\n",
      "   0.00000e+00 -3.99027e+00  4.63085e+00  0.00000e+00  3.26153e-01\n",
      "   7.97223e-08]]\n",
      "Episode 2350: log10(loss): -1.84337\n",
      "[[ 6.51113e-06  1.01682e+01  8.22214e-01  4.63040e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98962e+00  4.63040e+00  0.00000e+00  3.26126e-01\n",
      "   7.94499e-08]]\n",
      "Episode 2360: log10(loss): -1.84337\n",
      "[[ 6.47765e-06  1.01699e+01  8.22214e-01  4.63066e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98997e+00  4.63065e+00  0.00000e+00  3.26095e-01\n",
      "   7.89141e-08]]\n",
      "Episode 2370: log10(loss): -1.84337\n",
      "[[ 6.45063e-06  1.01708e+01  8.22214e-01  4.63061e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98991e+00  4.63061e+00  0.00000e+00  3.26062e-01\n",
      "   7.84870e-08]]\n",
      "Episode 2380: log10(loss): -1.84337\n",
      "[[ 6.42306e-06  1.01718e+01  8.22214e-01  4.63061e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98991e+00  4.63062e+00  0.00000e+00  3.26031e-01\n",
      "   7.80525e-08]]\n",
      "Episode 2390: log10(loss): -1.84337\n",
      "[[ 6.39631e-06  1.01727e+01  8.22214e-01  4.63060e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98988e+00  4.63059e+00  0.00000e+00  3.26001e-01\n",
      "   7.76317e-08]]\n",
      "Episode 2400: log10(loss): -1.84337\n",
      "[[ 6.36832e-06  1.01737e+01  8.22214e-01  4.63065e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98996e+00  4.63064e+00  0.00000e+00  3.25972e-01\n",
      "   7.71899e-08]]\n",
      "Episode 2410: log10(loss): -1.84338\n",
      "[[ 6.34225e-06  1.01746e+01  8.22214e-01  4.63062e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98990e+00  4.63061e+00  0.00000e+00  3.25942e-01\n",
      "   7.67824e-08]]\n",
      "Episode 2420: log10(loss): -1.84338\n",
      "[[ 6.31554e-06  1.01756e+01  8.22214e-01  4.63062e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98991e+00  4.63062e+00  0.00000e+00  3.25911e-01\n",
      "   7.63636e-08]]\n",
      "Episode 2430: log10(loss): -1.84338\n",
      "[[ 6.28868e-06  1.01766e+01  8.22214e-01  4.63065e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98994e+00  4.63064e+00  0.00000e+00  3.25882e-01\n",
      "   7.59434e-08]]\n",
      "Episode 2440: log10(loss): -1.84338\n",
      "[[ 6.26305e-06  1.01774e+01  8.22214e-01  4.63062e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98991e+00  4.63062e+00  0.00000e+00  3.25853e-01\n",
      "   7.55449e-08]]\n",
      "Episode 2450: log10(loss): -1.84338\n",
      "[[ 6.23617e-06  1.01785e+01  8.22214e-01  4.63067e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98997e+00  4.63067e+00  0.00000e+00  3.25823e-01\n",
      "   7.51243e-08]]\n",
      "Episode 2460: log10(loss): -1.84338\n",
      "[[ 6.21082e-06  1.01793e+01  8.22214e-01  4.63066e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98995e+00  4.63065e+00  0.00000e+00  3.25794e-01\n",
      "   7.47299e-08]]\n",
      "Episode 2470: log10(loss): -1.84338\n",
      "[[ 6.18574e-06  1.01802e+01  8.22214e-01  4.63063e+00  2.00015e-01\n",
      "   0.00000e+00 -3.98991e+00  4.63063e+00  0.00000e+00  3.25765e-01\n",
      "   7.43414e-08]]\n",
      "Episode 2480: log10(loss): -1.84338\n",
      "[[ 6.15778e-06  1.01815e+01  8.22214e-01  4.63078e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99012e+00  4.63077e+00  0.00000e+00  3.25737e-01\n",
      "   7.39036e-08]]\n",
      "Episode 2490: log10(loss): -1.84338\n",
      "[[ 6.13512e-06  1.01820e+01  8.22214e-01  4.63066e+00  2.00014e-01\n",
      "   0.00000e+00 -3.98995e+00  4.63066e+00  0.00000e+00  3.25709e-01\n",
      "   7.35555e-08]]\n",
      "Episode 2500: log10(loss): -1.84338\n",
      "[[ 6.10996e-06  1.01829e+01  8.22214e-01  4.63069e+00  2.00014e-01\n",
      "   0.00000e+00 -3.98998e+00  4.63068e+00  0.00000e+00  3.25682e-01\n",
      "   7.31656e-08]]\n",
      "Episode 2510: log10(loss): -1.84338\n",
      "[[ 6.08628e-06  1.01837e+01  8.22214e-01  4.63064e+00  2.00014e-01\n",
      "   0.00000e+00 -3.98992e+00  4.63064e+00  0.00000e+00  3.25654e-01\n",
      "   7.28004e-08]]\n",
      "Episode 2520: log10(loss): -1.84338\n",
      "[[ 6.06123e-06  1.01846e+01  8.22214e-01  4.63067e+00  2.00014e-01\n",
      "   0.00000e+00 -3.98996e+00  4.63067e+00  0.00000e+00  3.25626e-01\n",
      "   7.24144e-08]]\n",
      "Episode 2530: log10(loss): -1.84338\n",
      "[[ 6.03619e-06  1.01856e+01  8.22214e-01  4.63072e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99003e+00  4.63072e+00  0.00000e+00  3.25599e-01\n",
      "   7.20269e-08]]\n",
      "Episode 2540: log10(loss): -1.84338\n",
      "[[ 6.01304e-06  1.01863e+01  8.22214e-01  4.63069e+00  2.00014e-01\n",
      "   0.00000e+00 -3.98998e+00  4.63068e+00  0.00000e+00  3.25572e-01\n",
      "   7.16716e-08]]\n",
      "Episode 2550: log10(loss): -1.84338\n",
      "[[ 5.98976e-06  1.01871e+01  8.22214e-01  4.63067e+00  2.00014e-01\n",
      "   0.00000e+00 -3.98994e+00  4.63066e+00  0.00000e+00  3.25546e-01\n",
      "   7.13140e-08]]\n",
      "Episode 2560: log10(loss): -1.84339\n",
      "[[ 5.96538e-06  1.01881e+01  8.22214e-01  4.63072e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99002e+00  4.63072e+00  0.00000e+00  3.25521e-01\n",
      "   7.09380e-08]]\n",
      "Episode 2570: log10(loss): -1.84339\n",
      "[[ 5.94200e-06  1.01889e+01  8.22214e-01  4.63073e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99003e+00  4.63073e+00  0.00000e+00  3.25495e-01\n",
      "   7.05787e-08]]\n",
      "Episode 2580: log10(loss): -1.84339\n",
      "[[ 5.91951e-06  1.01896e+01  8.22214e-01  4.63071e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99000e+00  4.63071e+00  0.00000e+00  3.25470e-01\n",
      "   7.02337e-08]]\n",
      "Episode 2590: log10(loss): -1.84339\n",
      "[[ 5.89636e-06  1.01904e+01  8.22214e-01  4.63073e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99002e+00  4.63072e+00  0.00000e+00  3.25445e-01\n",
      "   6.98797e-08]]\n",
      "Episode 2600: log10(loss): -1.84339\n",
      "[[ 5.87413e-06  1.01912e+01  8.22214e-01  4.63070e+00  2.00014e-01\n",
      "   0.00000e+00 -3.98999e+00  4.63070e+00  0.00000e+00  3.25420e-01\n",
      "   6.95412e-08]]\n",
      "Episode 2610: log10(loss): -1.84339\n",
      "[[ 5.85072e-06  1.01921e+01  8.22214e-01  4.63074e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99003e+00  4.63074e+00  0.00000e+00  3.25393e-01\n",
      "   6.91836e-08]]\n",
      "Episode 2620: log10(loss): -1.84339\n",
      "[[ 5.82592e-06  1.01933e+01  8.22214e-01  4.63089e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99024e+00  4.63088e+00  0.00000e+00  3.25371e-01\n",
      "   6.88012e-08]]\n",
      "Episode 2630: log10(loss): -1.84339\n",
      "[[ 5.79751e-06  1.01950e+01  8.22214e-01  4.63122e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99071e+00  4.63122e+00  0.00000e+00  3.25348e-01\n",
      "   6.83596e-08]]\n",
      "Episode 2640: log10(loss): -1.84339\n",
      "[[ 5.78525e-06  1.01943e+01  8.22214e-01  4.63077e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99006e+00  4.63076e+00  0.00000e+00  3.25325e-01\n",
      "   6.81806e-08]]\n",
      "Episode 2650: log10(loss): -1.84339\n",
      "[[ 5.76255e-06  1.01953e+01  8.22214e-01  4.63077e+00  2.00014e-01\n",
      "   0.00000e+00 -3.99008e+00  4.63077e+00  0.00000e+00  3.25295e-01\n",
      "   6.78355e-08]]\n",
      "Episode 2660: log10(loss): -1.84339\n",
      "[[ 5.73981e-06  1.01963e+01  8.22214e-01  4.63081e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99012e+00  4.63081e+00  0.00000e+00  3.25268e-01\n",
      "   6.74895e-08]]\n",
      "Episode 2670: log10(loss): -1.84339\n",
      "[[ 5.71802e-06  1.01971e+01  8.22214e-01  4.63081e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99013e+00  4.63080e+00  0.00000e+00  3.25241e-01\n",
      "   6.71593e-08]]\n",
      "Episode 2680: log10(loss): -1.84339\n",
      "[[ 5.69597e-06  1.01980e+01  8.22214e-01  4.63082e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99014e+00  4.63083e+00  0.00000e+00  3.25213e-01\n",
      "   6.68247e-08]]\n",
      "Episode 2690: log10(loss): -1.84339\n",
      "[[ 5.67456e-06  1.01988e+01  8.22214e-01  4.63083e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99015e+00  4.63083e+00  0.00000e+00  3.25187e-01\n",
      "   6.65005e-08]]\n",
      "Episode 2700: log10(loss): -1.84339\n",
      "[[ 5.65464e-06  1.01994e+01  8.22214e-01  4.63077e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99006e+00  4.63076e+00  0.00000e+00  3.25162e-01\n",
      "   6.62006e-08]]\n",
      "Episode 2710: log10(loss): -1.84339\n",
      "[[ 5.63341e-06  1.02003e+01  8.22214e-01  4.63080e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99010e+00  4.63080e+00  0.00000e+00  3.25139e-01\n",
      "   6.58775e-08]]\n",
      "Episode 2720: log10(loss): -1.84339\n",
      "[[ 5.61352e-06  1.02009e+01  8.22214e-01  4.63077e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99005e+00  4.63076e+00  0.00000e+00  3.25115e-01\n",
      "   6.55771e-08]]\n",
      "Episode 2730: log10(loss): -1.84339\n",
      "[[ 5.59277e-06  1.02017e+01  8.22214e-01  4.63079e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99009e+00  4.63079e+00  0.00000e+00  3.25093e-01\n",
      "   6.52612e-08]]\n",
      "Episode 2740: log10(loss): -1.84340\n",
      "[[ 5.57240e-06  1.02024e+01  8.22214e-01  4.63080e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99010e+00  4.63079e+00  0.00000e+00  3.25071e-01\n",
      "   6.49537e-08]]\n",
      "Episode 2750: log10(loss): -1.84340\n",
      "[[ 5.55308e-06  1.02030e+01  8.22214e-01  4.63076e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99003e+00  4.63075e+00  0.00000e+00  3.25047e-01\n",
      "   6.46637e-08]]\n",
      "Episode 2760: log10(loss): -1.84340\n",
      "[[ 5.53309e-06  1.02038e+01  8.22214e-01  4.63078e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99006e+00  4.63077e+00  0.00000e+00  3.25026e-01\n",
      "   6.43595e-08]]\n",
      "Episode 2770: log10(loss): -1.84340\n",
      "[[ 5.51538e-06  1.02041e+01  8.22214e-01  4.63067e+00  2.00013e-01\n",
      "   0.00000e+00 -3.98991e+00  4.63066e+00  0.00000e+00  3.25003e-01\n",
      "   6.40946e-08]]\n",
      "Episode 2780: log10(loss): -1.84340\n",
      "[[ 5.50439e-06  1.02035e+01  8.22214e-01  4.63020e+00  2.00013e-01\n",
      "   0.00000e+00 -3.98924e+00  4.63020e+00  0.00000e+00  3.24980e-01\n",
      "   6.39373e-08]]\n",
      "Episode 2790: log10(loss): -1.84340\n",
      "[[ 5.47308e-06  1.02064e+01  8.22214e-01  4.63079e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99008e+00  4.63079e+00  0.00000e+00  3.24944e-01\n",
      "   6.34531e-08]]\n",
      "Episode 2800: log10(loss): -1.84340\n",
      "[[ 5.45144e-06  1.02072e+01  8.22214e-01  4.63099e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99035e+00  4.63098e+00  0.00000e+00  3.24935e-01\n",
      "   6.31247e-08]]\n",
      "Episode 2810: log10(loss): -1.84340\n",
      "[[ 5.43315e-06  1.02078e+01  8.22214e-01  4.63094e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99028e+00  4.63093e+00  0.00000e+00  3.24912e-01\n",
      "   6.28502e-08]]\n",
      "Episode 2820: log10(loss): -1.84340\n",
      "[[ 5.41491e-06  1.02084e+01  8.22214e-01  4.63089e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99021e+00  4.63088e+00  0.00000e+00  3.24889e-01\n",
      "   6.25778e-08]]\n",
      "Episode 2830: log10(loss): -1.84340\n",
      "[[ 5.39501e-06  1.02092e+01  8.22214e-01  4.63092e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99025e+00  4.63092e+00  0.00000e+00  3.24865e-01\n",
      "   6.22795e-08]]\n",
      "Episode 2840: log10(loss): -1.84340\n",
      "[[ 5.37621e-06  1.02099e+01  8.22214e-01  4.63091e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99024e+00  4.63091e+00  0.00000e+00  3.24842e-01\n",
      "   6.19984e-08]]\n",
      "Episode 2850: log10(loss): -1.84340\n",
      "[[ 5.35807e-06  1.02105e+01  8.22214e-01  4.63089e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99021e+00  4.63089e+00  0.00000e+00  3.24821e-01\n",
      "   6.17277e-08]]\n",
      "Episode 2860: log10(loss): -1.84340\n",
      "[[ 5.33948e-06  1.02112e+01  8.22214e-01  4.63089e+00  2.00013e-01\n",
      "   0.00000e+00 -3.99021e+00  4.63089e+00  0.00000e+00  3.24799e-01\n",
      "   6.14503e-08]]\n",
      "Episode 2870: log10(loss): -1.84340\n",
      "[[ 5.31986e-06  1.02121e+01  8.22214e-01  4.63096e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99030e+00  4.63095e+00  0.00000e+00  3.24777e-01\n",
      "   6.11557e-08]]\n",
      "Episode 2880: log10(loss): -1.84340\n",
      "[[ 5.30232e-06  1.02127e+01  8.22214e-01  4.63092e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99024e+00  4.63092e+00  0.00000e+00  3.24755e-01\n",
      "   6.08950e-08]]\n",
      "Episode 2890: log10(loss): -1.84340\n",
      "[[ 5.28461e-06  1.02133e+01  8.22214e-01  4.63089e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99020e+00  4.63090e+00  0.00000e+00  3.24734e-01\n",
      "   6.06318e-08]]\n",
      "Episode 2900: log10(loss): -1.84340\n",
      "[[ 5.26607e-06  1.02140e+01  8.22214e-01  4.63094e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99027e+00  4.63094e+00  0.00000e+00  3.24715e-01\n",
      "   6.03548e-08]]\n",
      "Episode 2910: log10(loss): -1.84340\n",
      "[[ 5.24769e-06  1.02148e+01  8.22214e-01  4.63096e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99030e+00  4.63096e+00  0.00000e+00  3.24693e-01\n",
      "   6.00815e-08]]\n",
      "Episode 2920: log10(loss): -1.84340\n",
      "[[ 5.22975e-06  1.02154e+01  8.22214e-01  4.63098e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99033e+00  4.63098e+00  0.00000e+00  3.24673e-01\n",
      "   5.98145e-08]]\n",
      "Episode 2930: log10(loss): -1.84340\n",
      "[[ 5.21250e-06  1.02160e+01  8.22214e-01  4.63097e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99031e+00  4.63097e+00  0.00000e+00  3.24654e-01\n",
      "   5.95587e-08]]\n",
      "Episode 2940: log10(loss): -1.84340\n",
      "[[ 5.19551e-06  1.02166e+01  8.22214e-01  4.63094e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99027e+00  4.63094e+00  0.00000e+00  3.24634e-01\n",
      "   5.93074e-08]]\n",
      "Episode 2950: log10(loss): -1.84341\n",
      "[[ 5.17811e-06  1.02172e+01  8.22214e-01  4.63096e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99029e+00  4.63096e+00  0.00000e+00  3.24614e-01\n",
      "   5.90493e-08]]\n",
      "Episode 2960: log10(loss): -1.84341\n",
      "[[ 5.16086e-06  1.02178e+01  8.22214e-01  4.63097e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99030e+00  4.63097e+00  0.00000e+00  3.24596e-01\n",
      "   5.87937e-08]]\n",
      "Episode 2970: log10(loss): -1.84341\n",
      "[[ 5.14337e-06  1.02185e+01  8.22214e-01  4.63100e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99034e+00  4.63100e+00  0.00000e+00  3.24577e-01\n",
      "   5.85345e-08]]\n",
      "Episode 2980: log10(loss): -1.84341\n",
      "[[ 5.12697e-06  1.02190e+01  8.22214e-01  4.63098e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99032e+00  4.63098e+00  0.00000e+00  3.24559e-01\n",
      "   5.82929e-08]]\n",
      "Episode 2990: log10(loss): -1.84341\n",
      "[[ 5.11053e-06  1.02196e+01  8.22214e-01  4.63097e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99030e+00  4.63096e+00  0.00000e+00  3.24541e-01\n",
      "   5.80504e-08]]\n",
      "Episode 3000: log10(loss): -1.84341\n",
      "[[ 5.09283e-06  1.02203e+01  8.22214e-01  4.63103e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99039e+00  4.63103e+00  0.00000e+00  3.24523e-01\n",
      "   5.77888e-08]]\n",
      "Episode 3010: log10(loss): -1.84341\n",
      "[[ 5.07623e-06  1.02210e+01  8.22214e-01  4.63106e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99042e+00  4.63105e+00  0.00000e+00  3.24505e-01\n",
      "   5.75414e-08]]\n",
      "Episode 3020: log10(loss): -1.84341\n",
      "[[ 5.06112e-06  1.02213e+01  8.22214e-01  4.63100e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99034e+00  4.63100e+00  0.00000e+00  3.24489e-01\n",
      "   5.73200e-08]]\n",
      "Episode 3030: log10(loss): -1.84341\n",
      "[[ 5.04461e-06  1.02219e+01  8.22214e-01  4.63102e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99036e+00  4.63102e+00  0.00000e+00  3.24471e-01\n",
      "   5.70761e-08]]\n",
      "Episode 3040: log10(loss): -1.84341\n",
      "[[ 5.02824e-06  1.02225e+01  8.22214e-01  4.63104e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99040e+00  4.63104e+00  0.00000e+00  3.24455e-01\n",
      "   5.68349e-08]]\n",
      "Episode 3050: log10(loss): -1.84341\n",
      "[[ 5.01156e-06  1.02232e+01  8.22214e-01  4.63110e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99048e+00  4.63110e+00  0.00000e+00  3.24439e-01\n",
      "   5.65881e-08]]\n",
      "Episode 3060: log10(loss): -1.84341\n",
      "[[ 4.98968e-06  1.02248e+01  8.22214e-01  4.63147e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99100e+00  4.63147e+00  0.00000e+00  3.24424e-01\n",
      "   5.62598e-08]]\n",
      "Episode 3070: log10(loss): -1.84340\n",
      "[[ 4.86627e-06  1.02455e+01  8.22214e-01  4.63761e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99968e+00  4.63760e+00  0.00000e+00  3.24351e-01\n",
      "   5.43550e-08]]\n",
      "Episode 3080: log10(loss): -1.84341\n",
      "[[ 4.95244e-06  1.02273e+01  8.22214e-01  4.63183e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99150e+00  4.63183e+00  0.00000e+00  3.24377e-01\n",
      "   5.57008e-08]]\n",
      "Episode 3090: log10(loss): -1.84341\n",
      "[[ 4.94963e-06  1.02254e+01  8.22214e-01  4.63120e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99061e+00  4.63119e+00  0.00000e+00  3.24379e-01\n",
      "   5.56658e-08]]\n",
      "Episode 3100: log10(loss): -1.84341\n",
      "[[ 4.93676e-06  1.02254e+01  8.22214e-01  4.63107e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99043e+00  4.63107e+00  0.00000e+00  3.24364e-01\n",
      "   5.54756e-08]]\n",
      "Episode 3110: log10(loss): -1.84341\n",
      "[[ 4.92222e-06  1.02259e+01  8.22214e-01  4.63104e+00  2.00012e-01\n",
      "   0.00000e+00 -3.99038e+00  4.63104e+00  0.00000e+00  3.24348e-01\n",
      "   5.52593e-08]]\n",
      "Episode 3120: log10(loss): -1.84341\n",
      "[[ 4.90743e-06  1.02263e+01  8.22214e-01  4.63103e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99037e+00  4.63103e+00  0.00000e+00  3.24332e-01\n",
      "   5.50392e-08]]\n",
      "Episode 3130: log10(loss): -1.84341\n",
      "[[ 4.89190e-06  1.02270e+01  8.22214e-01  4.63107e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99042e+00  4.63107e+00  0.00000e+00  3.24315e-01\n",
      "   5.48079e-08]]\n",
      "Episode 3140: log10(loss): -1.84341\n",
      "[[ 4.87775e-06  1.02274e+01  8.22214e-01  4.63103e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99036e+00  4.63103e+00  0.00000e+00  3.24299e-01\n",
      "   5.45984e-08]]\n",
      "Episode 3150: log10(loss): -1.84341\n",
      "[[ 4.86235e-06  1.02280e+01  8.22214e-01  4.63107e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99042e+00  4.63107e+00  0.00000e+00  3.24283e-01\n",
      "   5.43692e-08]]\n",
      "Episode 3160: log10(loss): -1.84341\n",
      "[[ 4.84730e-06  1.02286e+01  8.22214e-01  4.63111e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99047e+00  4.63110e+00  0.00000e+00  3.24267e-01\n",
      "   5.41453e-08]]\n",
      "Episode 3170: log10(loss): -1.84341\n",
      "[[ 4.83329e-06  1.02290e+01  8.22214e-01  4.63108e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99043e+00  4.63107e+00  0.00000e+00  3.24252e-01\n",
      "   5.39384e-08]]\n",
      "Episode 3180: log10(loss): -1.84341\n",
      "[[ 4.81820e-06  1.02296e+01  8.22214e-01  4.63112e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99049e+00  4.63112e+00  0.00000e+00  3.24236e-01\n",
      "   5.37145e-08]]\n",
      "Episode 3190: log10(loss): -1.84341\n",
      "[[ 4.80450e-06  1.02300e+01  8.22214e-01  4.63110e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99045e+00  4.63109e+00  0.00000e+00  3.24222e-01\n",
      "   5.35121e-08]]\n",
      "Episode 3200: log10(loss): -1.84341\n",
      "[[ 4.79044e-06  1.02305e+01  8.22214e-01  4.63108e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99043e+00  4.63109e+00  0.00000e+00  3.24206e-01\n",
      "   5.33047e-08]]\n",
      "Episode 3210: log10(loss): -1.84342\n",
      "[[ 4.77653e-06  1.02309e+01  8.22214e-01  4.63109e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99044e+00  4.63108e+00  0.00000e+00  3.24193e-01\n",
      "   5.30994e-08]]\n",
      "Episode 3220: log10(loss): -1.84342\n",
      "[[ 4.76268e-06  1.02313e+01  8.22214e-01  4.63107e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99042e+00  4.63107e+00  0.00000e+00  3.24178e-01\n",
      "   5.28962e-08]]\n",
      "Episode 3230: log10(loss): -1.84342\n",
      "[[ 4.74841e-06  1.02319e+01  8.22214e-01  4.63110e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99045e+00  4.63110e+00  0.00000e+00  3.24163e-01\n",
      "   5.26859e-08]]\n",
      "Episode 3240: log10(loss): -1.84342\n",
      "[[ 4.73443e-06  1.02324e+01  8.22214e-01  4.63111e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99047e+00  4.63111e+00  0.00000e+00  3.24148e-01\n",
      "   5.24802e-08]]\n",
      "Episode 3250: log10(loss): -1.84342\n",
      "[[ 4.72053e-06  1.02329e+01  8.22214e-01  4.63112e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99048e+00  4.63112e+00  0.00000e+00  3.24133e-01\n",
      "   5.22767e-08]]\n",
      "Episode 3260: log10(loss): -1.84342\n",
      "[[ 4.70634e-06  1.02334e+01  8.22214e-01  4.63115e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99053e+00  4.63115e+00  0.00000e+00  3.24118e-01\n",
      "   5.20686e-08]]\n",
      "Episode 3270: log10(loss): -1.84342\n",
      "[[ 4.69424e-06  1.02336e+01  8.22214e-01  4.63107e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99041e+00  4.63107e+00  0.00000e+00  3.24106e-01\n",
      "   5.18921e-08]]\n",
      "Episode 3280: log10(loss): -1.84342\n",
      "[[ 4.67996e-06  1.02342e+01  8.22214e-01  4.63112e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99048e+00  4.63112e+00  0.00000e+00  3.24092e-01\n",
      "   5.16826e-08]]\n",
      "Episode 3290: log10(loss): -1.84342\n",
      "[[ 4.66626e-06  1.02347e+01  8.22214e-01  4.63114e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99051e+00  4.63115e+00  0.00000e+00  3.24077e-01\n",
      "   5.14821e-08]]\n",
      "Episode 3300: log10(loss): -1.84342\n",
      "[[ 4.65324e-06  1.02351e+01  8.22214e-01  4.63113e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99048e+00  4.63112e+00  0.00000e+00  3.24063e-01\n",
      "   5.12934e-08]]\n",
      "Episode 3310: log10(loss): -1.84342\n",
      "[[ 4.63988e-06  1.02356e+01  8.22214e-01  4.63113e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99049e+00  4.63113e+00  0.00000e+00  3.24049e-01\n",
      "   5.10990e-08]]\n",
      "Episode 3320: log10(loss): -1.84342\n",
      "[[ 4.62741e-06  1.02358e+01  8.22214e-01  4.63109e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99044e+00  4.63110e+00  0.00000e+00  3.24037e-01\n",
      "   5.09171e-08]]\n",
      "Episode 3330: log10(loss): -1.84342\n",
      "[[ 4.61610e-06  1.02360e+01  8.22214e-01  4.63099e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99029e+00  4.63099e+00  0.00000e+00  3.24023e-01\n",
      "   5.07536e-08]]\n",
      "Episode 3340: log10(loss): -1.84342\n",
      "[[ 4.59898e-06  1.02372e+01  8.22214e-01  4.63122e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99062e+00  4.63123e+00  0.00000e+00  3.24004e-01\n",
      "   5.05022e-08]]\n",
      "Episode 3350: log10(loss): -1.84342\n",
      "[[ 4.58532e-06  1.02378e+01  8.22214e-01  4.63129e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99071e+00  4.63128e+00  0.00000e+00  3.23993e-01\n",
      "   5.03030e-08]]\n",
      "Episode 3360: log10(loss): -1.84342\n",
      "[[ 4.57715e-06  1.02373e+01  8.22214e-01  4.63102e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99032e+00  4.63101e+00  0.00000e+00  3.23983e-01\n",
      "   5.01873e-08]]\n",
      "Episode 3370: log10(loss): -1.84342\n",
      "[[ 4.56194e-06  1.02383e+01  8.22214e-01  4.63114e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99049e+00  4.63113e+00  0.00000e+00  3.23963e-01\n",
      "   4.99648e-08]]\n",
      "Episode 3380: log10(loss): -1.84342\n",
      "[[ 4.54087e-06  1.02404e+01  8.22214e-01  4.63165e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99122e+00  4.63165e+00  0.00000e+00  3.23946e-01\n",
      "   4.96536e-08]]\n",
      "Episode 3390: log10(loss): -1.84342\n",
      "[[ 4.53859e-06  1.02386e+01  8.22214e-01  4.63108e+00  2.00011e-01\n",
      "   0.00000e+00 -3.99041e+00  4.63107e+00  0.00000e+00  3.23948e-01\n",
      "   4.96274e-08]]\n",
      "Episode 3400: log10(loss): -1.84342\n",
      "[[ 4.55149e-06  1.02340e+01  8.22214e-01  4.62950e+00  2.00011e-01\n",
      "   0.00000e+00 -3.98818e+00  4.62950e+00  0.00000e+00  3.23944e-01\n",
      "   4.98310e-08]]\n",
      "Episode 3410: log10(loss): -1.84341\n",
      "[[ 4.52007e-06  1.02388e+01  8.22214e-01  4.63044e+00  2.00011e-01\n",
      "   0.00000e+00 -3.98951e+00  4.63045e+00  0.00000e+00  3.23882e-01\n",
      "   4.93652e-08]]\n",
      "Episode 3420: log10(loss): -1.84342\n",
      "[[ 4.37693e-06  1.02663e+01  8.22214e-01  4.63851e+00  2.00010e-01\n",
      "   0.00000e+00 -4.00092e+00  4.63851e+00  0.00000e+00  3.23780e-01\n",
      "   4.72220e-08]]\n",
      "Episode 3430: log10(loss): -1.84342\n",
      "[[ 4.48282e-06  1.02423e+01  8.22214e-01  4.63138e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99084e+00  4.63138e+00  0.00000e+00  3.23860e-01\n",
      "   4.88121e-08]]\n",
      "Episode 3440: log10(loss): -1.84342\n",
      "[[ 4.47326e-06  1.02420e+01  8.22214e-01  4.63136e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99081e+00  4.63136e+00  0.00000e+00  3.23868e-01\n",
      "   4.86674e-08]]\n",
      "Episode 3450: log10(loss): -1.84342\n",
      "[[ 4.46433e-06  1.02419e+01  8.22214e-01  4.63119e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99056e+00  4.63118e+00  0.00000e+00  3.23855e-01\n",
      "   4.85343e-08]]\n",
      "Episode 3460: log10(loss): -1.84342\n",
      "[[ 4.45184e-06  1.02425e+01  8.22214e-01  4.63122e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99060e+00  4.63122e+00  0.00000e+00  3.23839e-01\n",
      "   4.83481e-08]]\n",
      "Episode 3470: log10(loss): -1.84342\n",
      "[[ 4.44075e-06  1.02428e+01  8.22214e-01  4.63119e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99056e+00  4.63118e+00  0.00000e+00  3.23826e-01\n",
      "   4.81836e-08]]\n",
      "Episode 3480: log10(loss): -1.84342\n",
      "[[ 4.42873e-06  1.02433e+01  8.22214e-01  4.63121e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99059e+00  4.63121e+00  0.00000e+00  3.23813e-01\n",
      "   4.80045e-08]]\n",
      "Episode 3490: log10(loss): -1.84342\n",
      "[[ 4.41734e-06  1.02437e+01  8.22214e-01  4.63120e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99057e+00  4.63120e+00  0.00000e+00  3.23800e-01\n",
      "   4.78349e-08]]\n",
      "Episode 3500: log10(loss): -1.84342\n",
      "[[ 4.40517e-06  1.02442e+01  8.22214e-01  4.63124e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99062e+00  4.63123e+00  0.00000e+00  3.23785e-01\n",
      "   4.76552e-08]]\n",
      "Episode 3510: log10(loss): -1.84342\n",
      "[[ 4.39425e-06  1.02445e+01  8.22214e-01  4.63121e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99059e+00  4.63121e+00  0.00000e+00  3.23773e-01\n",
      "   4.74928e-08]]\n",
      "Episode 3520: log10(loss): -1.84342\n",
      "[[ 4.38289e-06  1.02450e+01  8.22214e-01  4.63121e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99058e+00  4.63121e+00  0.00000e+00  3.23759e-01\n",
      "   4.73243e-08]]\n",
      "Episode 3530: log10(loss): -1.84343\n",
      "[[ 4.37129e-06  1.02454e+01  8.22214e-01  4.63123e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99061e+00  4.63122e+00  0.00000e+00  3.23745e-01\n",
      "   4.71532e-08]]\n",
      "Episode 3540: log10(loss): -1.84343\n",
      "[[ 4.35967e-06  1.02459e+01  8.22214e-01  4.63124e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99063e+00  4.63124e+00  0.00000e+00  3.23732e-01\n",
      "   4.69818e-08]]\n",
      "Episode 3550: log10(loss): -1.84343\n",
      "[[ 4.34829e-06  1.02464e+01  8.22214e-01  4.63126e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99065e+00  4.63126e+00  0.00000e+00  3.23719e-01\n",
      "   4.68142e-08]]\n",
      "Episode 3560: log10(loss): -1.84343\n",
      "[[ 4.33709e-06  1.02468e+01  8.22214e-01  4.63125e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99064e+00  4.63125e+00  0.00000e+00  3.23705e-01\n",
      "   4.66502e-08]]\n",
      "Episode 3570: log10(loss): -1.84343\n",
      "[[ 4.32606e-06  1.02472e+01  8.22214e-01  4.63124e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99062e+00  4.63124e+00  0.00000e+00  3.23690e-01\n",
      "   4.64888e-08]]\n",
      "Episode 3580: log10(loss): -1.84343\n",
      "[[ 4.31506e-06  1.02476e+01  8.22214e-01  4.63123e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99061e+00  4.63124e+00  0.00000e+00  3.23677e-01\n",
      "   4.63275e-08]]\n",
      "Episode 3590: log10(loss): -1.84343\n",
      "[[ 4.30434e-06  1.02480e+01  8.22214e-01  4.63122e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99059e+00  4.63122e+00  0.00000e+00  3.23664e-01\n",
      "   4.61702e-08]]\n",
      "Episode 3600: log10(loss): -1.84343\n",
      "[[ 4.29330e-06  1.02484e+01  8.22214e-01  4.63125e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99064e+00  4.63125e+00  0.00000e+00  3.23654e-01\n",
      "   4.60059e-08]]\n",
      "Episode 3610: log10(loss): -1.84343\n",
      "[[ 4.28225e-06  1.02489e+01  8.22214e-01  4.63127e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99066e+00  4.63127e+00  0.00000e+00  3.23641e-01\n",
      "   4.58433e-08]]\n",
      "Episode 3620: log10(loss): -1.84343\n",
      "[[ 4.27179e-06  1.02492e+01  8.22214e-01  4.63125e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99064e+00  4.63125e+00  0.00000e+00  3.23630e-01\n",
      "   4.56900e-08]]\n",
      "Episode 3630: log10(loss): -1.84343\n",
      "[[ 4.26033e-06  1.02498e+01  8.22214e-01  4.63130e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99071e+00  4.63130e+00  0.00000e+00  3.23617e-01\n",
      "   4.55214e-08]]\n",
      "Episode 3640: log10(loss): -1.84343\n",
      "[[ 4.25040e-06  1.02500e+01  8.22214e-01  4.63125e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99063e+00  4.63125e+00  0.00000e+00  3.23604e-01\n",
      "   4.53763e-08]]\n",
      "Episode 3650: log10(loss): -1.84343\n",
      "[[ 4.23988e-06  1.02504e+01  8.22214e-01  4.63127e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99066e+00  4.63127e+00  0.00000e+00  3.23594e-01\n",
      "   4.52211e-08]]\n",
      "Episode 3660: log10(loss): -1.84343\n",
      "[[ 4.22954e-06  1.02507e+01  8.22214e-01  4.63126e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99065e+00  4.63125e+00  0.00000e+00  3.23582e-01\n",
      "   4.50698e-08]]\n",
      "Episode 3670: log10(loss): -1.84343\n",
      "[[ 4.21863e-06  1.02512e+01  8.22214e-01  4.63129e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99068e+00  4.63128e+00  0.00000e+00  3.23569e-01\n",
      "   4.49107e-08]]\n",
      "Episode 3680: log10(loss): -1.84343\n",
      "[[ 4.20825e-06  1.02516e+01  8.22214e-01  4.63128e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99067e+00  4.63127e+00  0.00000e+00  3.23556e-01\n",
      "   4.47597e-08]]\n",
      "Episode 3690: log10(loss): -1.84343\n",
      "[[ 4.19768e-06  1.02520e+01  8.22214e-01  4.63129e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99068e+00  4.63128e+00  0.00000e+00  3.23543e-01\n",
      "   4.46066e-08]]\n",
      "Episode 3700: log10(loss): -1.84343\n",
      "[[ 4.18736e-06  1.02524e+01  8.22214e-01  4.63129e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99068e+00  4.63129e+00  0.00000e+00  3.23532e-01\n",
      "   4.44558e-08]]\n",
      "Episode 3710: log10(loss): -1.84343\n",
      "[[ 4.17729e-06  1.02527e+01  8.22214e-01  4.63129e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99069e+00  4.63129e+00  0.00000e+00  3.23521e-01\n",
      "   4.43086e-08]]\n",
      "Episode 3720: log10(loss): -1.84343\n",
      "[[ 4.16714e-06  1.02531e+01  8.22214e-01  4.63128e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99067e+00  4.63128e+00  0.00000e+00  3.23509e-01\n",
      "   4.41618e-08]]\n",
      "Episode 3730: log10(loss): -1.84343\n",
      "[[ 4.15692e-06  1.02535e+01  8.22214e-01  4.63130e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99069e+00  4.63129e+00  0.00000e+00  3.23498e-01\n",
      "   4.40127e-08]]\n",
      "Episode 3740: log10(loss): -1.84343\n",
      "[[ 4.14679e-06  1.02539e+01  8.22214e-01  4.63129e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99068e+00  4.63129e+00  0.00000e+00  3.23484e-01\n",
      "   4.38667e-08]]\n",
      "Episode 3750: log10(loss): -1.84343\n",
      "[[ 4.13709e-06  1.02542e+01  8.22214e-01  4.63127e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99066e+00  4.63127e+00  0.00000e+00  3.23474e-01\n",
      "   4.37253e-08]]\n",
      "Episode 3760: log10(loss): -1.84343\n",
      "[[ 4.12711e-06  1.02545e+01  8.22214e-01  4.63128e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99067e+00  4.63129e+00  0.00000e+00  3.23464e-01\n",
      "   4.35796e-08]]\n",
      "Episode 3770: log10(loss): -1.84343\n",
      "[[ 4.11720e-06  1.02549e+01  8.22214e-01  4.63130e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99070e+00  4.63130e+00  0.00000e+00  3.23454e-01\n",
      "   4.34356e-08]]\n",
      "Episode 3780: log10(loss): -1.84343\n",
      "[[ 4.10769e-06  1.02552e+01  8.22214e-01  4.63128e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99066e+00  4.63128e+00  0.00000e+00  3.23443e-01\n",
      "   4.32978e-08]]\n",
      "Episode 3790: log10(loss): -1.84343\n",
      "[[ 4.09774e-06  1.02556e+01  8.22214e-01  4.63130e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99069e+00  4.63130e+00  0.00000e+00  3.23433e-01\n",
      "   4.31533e-08]]\n",
      "Episode 3800: log10(loss): -1.84343\n",
      "[[ 4.08797e-06  1.02559e+01  8.22214e-01  4.63131e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99070e+00  4.63131e+00  0.00000e+00  3.23422e-01\n",
      "   4.30114e-08]]\n",
      "Episode 3810: log10(loss): -1.84343\n",
      "[[ 4.07794e-06  1.02564e+01  8.22214e-01  4.63132e+00  2.00010e-01\n",
      "   0.00000e+00 -3.99072e+00  4.63132e+00  0.00000e+00  3.23410e-01\n",
      "   4.28674e-08]]\n",
      "Episode 3820: log10(loss): -1.84343\n",
      "[[ 4.06887e-06  1.02566e+01  8.22214e-01  4.63129e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99067e+00  4.63129e+00  0.00000e+00  3.23399e-01\n",
      "   4.27359e-08]]\n",
      "Episode 3830: log10(loss): -1.84343\n",
      "[[ 4.05898e-06  1.02570e+01  8.22214e-01  4.63131e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99070e+00  4.63130e+00  0.00000e+00  3.23388e-01\n",
      "   4.25939e-08]]\n",
      "Episode 3840: log10(loss): -1.84343\n",
      "[[ 4.04912e-06  1.02575e+01  8.22214e-01  4.63133e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99073e+00  4.63133e+00  0.00000e+00  3.23376e-01\n",
      "   4.24514e-08]]\n",
      "Episode 3850: log10(loss): -1.84343\n",
      "[[ 4.03972e-06  1.02578e+01  8.22214e-01  4.63131e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99071e+00  4.63131e+00  0.00000e+00  3.23364e-01\n",
      "   4.23170e-08]]\n",
      "Episode 3860: log10(loss): -1.84343\n",
      "[[ 4.03024e-06  1.02581e+01  8.22214e-01  4.63132e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99071e+00  4.63131e+00  0.00000e+00  3.23353e-01\n",
      "   4.21809e-08]]\n",
      "Episode 3870: log10(loss): -1.84343\n",
      "[[ 4.02079e-06  1.02585e+01  8.22214e-01  4.63132e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99071e+00  4.63132e+00  0.00000e+00  3.23342e-01\n",
      "   4.20453e-08]]\n",
      "Episode 3880: log10(loss): -1.84343\n",
      "[[ 4.01115e-06  1.02589e+01  8.22214e-01  4.63133e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99073e+00  4.63133e+00  0.00000e+00  3.23330e-01\n",
      "   4.19074e-08]]\n",
      "Episode 3890: log10(loss): -1.84343\n",
      "[[ 4.00181e-06  1.02593e+01  8.22214e-01  4.63134e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99074e+00  4.63133e+00  0.00000e+00  3.23319e-01\n",
      "   4.17736e-08]]\n",
      "Episode 3900: log10(loss): -1.84343\n",
      "[[ 3.99266e-06  1.02596e+01  8.22214e-01  4.63133e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99073e+00  4.63133e+00  0.00000e+00  3.23310e-01\n",
      "   4.16424e-08]]\n",
      "Episode 3910: log10(loss): -1.84343\n",
      "[[ 3.98367e-06  1.02599e+01  8.22214e-01  4.63132e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99072e+00  4.63132e+00  0.00000e+00  3.23299e-01\n",
      "   4.15139e-08]]\n",
      "Episode 3920: log10(loss): -1.84343\n",
      "[[ 3.97474e-06  1.02601e+01  8.22214e-01  4.63132e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99071e+00  4.63133e+00  0.00000e+00  3.23291e-01\n",
      "   4.13852e-08]]\n",
      "Episode 3930: log10(loss): -1.84344\n",
      "[[ 3.96565e-06  1.02605e+01  8.22214e-01  4.63133e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99073e+00  4.63133e+00  0.00000e+00  3.23281e-01\n",
      "   4.12550e-08]]\n",
      "Episode 3940: log10(loss): -1.84344\n",
      "[[ 3.95641e-06  1.02609e+01  8.22214e-01  4.63134e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99074e+00  4.63134e+00  0.00000e+00  3.23270e-01\n",
      "   4.11234e-08]]\n",
      "Episode 3950: log10(loss): -1.84344\n",
      "[[ 3.94717e-06  1.02613e+01  8.22214e-01  4.63134e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99074e+00  4.63135e+00  0.00000e+00  3.23258e-01\n",
      "   4.09921e-08]]\n",
      "Episode 3960: log10(loss): -1.84344\n",
      "[[ 3.93826e-06  1.02616e+01  8.22214e-01  4.63135e+00  2.00009e-01\n",
      "   0.00000e+00 -3.99076e+00  4.63135e+00  0.00000e+00  3.23249e-01\n",
      "   4.08646e-08]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Work\\DeepEquilibriumNets\\3g simple predict consumption.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20predict%20consumption.ipynb#ch0000033?line=0'>1</a>\u001b[0m x_final, loss_list \u001b[39m=\u001b[39m training_algorithm(x_start)\n",
      "\u001b[1;32md:\\Documents\\Work\\DeepEquilibriumNets\\3g simple predict consumption.ipynb Cell 33'\u001b[0m in \u001b[0;36mtraining_algorithm\u001b[1;34m(x_start)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20predict%20consumption.ipynb#ch0000032?line=12'>13</a>\u001b[0m X_episodes \u001b[39m=\u001b[39m simulate_episodes(x_start, len_episodes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20predict%20consumption.ipynb#ch0000032?line=14'>15</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20predict%20consumption.ipynb#ch0000032?line=15'>16</a>\u001b[0m loss_value \u001b[39m=\u001b[39m train_step(X_episodes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20predict%20consumption.ipynb#ch0000032?line=17'>18</a>\u001b[0m \u001b[39m# Update starting episode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20predict%20consumption.ipynb#ch0000032?line=18'>19</a>\u001b[0m x_start \u001b[39m=\u001b[39m X_episodes[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mreshape([\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_final, loss_list = training_algorithm(x_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss per episode\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.title(f\"log10(loss) by episode with learning rate {lr}\", color='white')\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Episode\", color='white')\n",
    "plt.ylabel(\"log10(loss)\", color='white')\n",
    "plt.tick_params(axis='x', colors='white')    #setting up X-axis tick color to red\n",
    "plt.tick_params(axis='y', colors='white') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights --71mins for 10000 episodes\n",
    "# model.save_weights('./tmpc/model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "# model.load_weights('./tmp/model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate(x_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model(x_final))\n",
    "# R_orig, R, ph_orig, ph, a_out, a_out_all, h_out_orig, h_out, c_orig, c, A_out, H_out\n",
    "# compute_economy(x_final, model(x_final))\n",
    "# compute_next_state(x_final, model(x_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate_episodes(x_final, 10000)                                            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16a72728a2753f090697b0ded7f8f44a91219d50bb29d7d94928e81972a6ec07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
