{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else cpu)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further setting up\n",
    "\n",
    "# Set the seed for replicable results\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Helper variable\n",
    "lb = 1e-3 # Define lower bound for c, h\n",
    "eps = 1e-5 # Define inverse punishment value for negative predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SCALAR PARAMETERS\n",
    "alpha = 0.1\n",
    "beta = 0.99**25\n",
    "gamma = 3\n",
    "J = 3\n",
    "jret = 2\n",
    "thetaret = 0.2\n",
    "delta = (1 + 0.015)**25 - 1\n",
    "Gmu = (1 + 0.005)**25\n",
    "Gchi = (1 + 0.02)**25\n",
    "Pic = (1 + 0.02)**25\n",
    "Hs = 0.79/5\n",
    "\n",
    "# COMPUTE PARAMETER TRANSFORMATIONS\n",
    "betahat = Gchi**((1-alpha)*(1-gamma))*beta\n",
    "Pih = Gchi * Pic\n",
    "\n",
    "# DEFINE AGE-VARYING PRODUCTIVITY AND SURVIVAL RATES\n",
    "chi1 = 0.874264\n",
    "mu1 = 0.493497\n",
    "\n",
    "chi = chi1 * np.array([1, 1.5])\n",
    "\n",
    "zeta = np.array([0.976163, 0.784336, 0])\n",
    "zeta_0 = np.concatenate(([1], zeta[0:2]))\n",
    "zeta_0 = torch.from_numpy(zeta_0).to(device)\n",
    "\n",
    "# COMPUTE POPULATION DISTRIBUTION\n",
    "mu = np.zeros(J)\n",
    "mu[0] = mu1\n",
    "for j in range(1, J):\n",
    "    mu[j] = (zeta[j-1] / Gmu) * mu[j-1]\n",
    "\n",
    "# COMPUTE TAX RATE\n",
    "tau = np.sum([thetaret*Gchi**(jret - j)*mu[j] for j in range(jret, J)]) / np.sum(mu[0:jret]*chi)\n",
    "mu = torch.from_numpy(mu).to(device)\n",
    "\n",
    "# COMPUTE AGE-VARYING INCOME\n",
    "y = torch.zeros(J)\n",
    "for j in range(J):\n",
    "    if j < jret:\n",
    "        y[j] = (1-tau) * chi[j]\n",
    "    else:\n",
    "        y[j] = thetaret * Gchi ** (jret-j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Neural network architecture parameters\n",
    "input_size = 2 + 3*J  # Dimension of extended state space (4 aggregate quantities and 3 distributions)\n",
    "output_size = J + J-1 + 2  # Output dimension: 3 for housing, 2 for assets, 2 aggregate quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "model = NeuralNetwork(input_size, output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(1, input_size))\n",
    "# X.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_output(output):\n",
    "    \"\"\"\n",
    "    Unpacks the output of the neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : tensor\n",
    "        Output of neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack simulated housing and financial assets\n",
    "    R_orig = torch.unsqueeze(output[:, 0], 1)\n",
    "    ph_orig = torch.unsqueeze(output[:, 1], 1)\n",
    "    a_out = output[:, 2:4]\n",
    "    h_out_orig = output[:, 4:7]\n",
    "\n",
    "    return R_orig, ph_orig, a_out, h_out_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack_output(model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_economy(X, output):\n",
    "    \"\"\"\n",
    "    Solves today's economy given state and output of neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack inputs and outputs\n",
    "    w = X[:, 2:5] # Distribution of tangible wealth\n",
    "    R_orig, ph_orig, a_out, h_out_orig = unpack_output(output)\n",
    "\n",
    "    # The network can predict negative values before it learns not to. \n",
    "    R = torch.maximum(R_orig, torch.ones_like(R_orig)*eps)\n",
    "    ph = torch.maximum(ph_orig, torch.ones_like(ph_orig)*eps)\n",
    "    h_out = torch.maximum(h_out_orig, torch.ones_like(h_out_orig)*lb)\n",
    "\n",
    "    # Compute savings for oldest cohort\n",
    "    a_out_J = -(Pih * ph * torch.unsqueeze(h_out[:, J-1], 1)) / R\n",
    "    a_out_all = torch.cat([a_out, a_out_J], dim=1)\n",
    "\n",
    "    # Compute consumption and impose non-negative constraint\n",
    "    c_orig = w - ph*(1+delta)*h_out - a_out_all\n",
    "    c = torch.maximum(c_orig, torch.ones_like(c_orig)*lb)\n",
    "\n",
    "    # Compute aggregate savings and housing\n",
    "    A_out = torch.sum(mu * a_out_all, dim=1, keepdims=True)\n",
    "    H_out = torch.sum(mu * h_out, dim=1, keepdims=True)\n",
    "\n",
    "    return R_orig, R, ph_orig, ph, a_out, a_out_all, h_out_orig, h_out, c_orig, c, A_out, H_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_economy(X, model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_state(X, output):\n",
    "    \"\"\"\n",
    "    Computes the state for the next period, given the outputs of the neural network today.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs : tensors\n",
    "        Outputs of neural network\n",
    "    \"\"\"\n",
    "\n",
    "    _, R, _, ph, a_out, _, _, h_out, _, _, _, _ = compute_economy(X, output)\n",
    "\n",
    "    m = torch.Tensor.size(X)[0]\n",
    "\n",
    "    a_prime = torch.cat([torch.zeros([m, 1]), a_out], dim=1)\n",
    "    h_prime = torch.cat([torch.zeros([m, 1]), h_out[:, 0:J-1]], dim=1)\n",
    "\n",
    "    # Distribution of tomorrow's financial wealth\n",
    "    w_prime = y + (R*a_prime + Pih*ph*h_prime) / (zeta_0*Gchi*Pic)\n",
    "\n",
    "    # Tomorrow's extended state: catenate the parts together\n",
    "    X_prime = torch.cat([\n",
    "        R,\n",
    "        ph,\n",
    "        w_prime,\n",
    "        a_prime,\n",
    "        h_prime\n",
    "        ], dim=1)        \n",
    "\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_next_state(X, model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_period(X):\n",
    "    \"\"\"\n",
    "    Evaluate neural network given today's state and generate tomorrow's state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : tensor\n",
    "        Today's extended state\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        Tomorrow's extended state\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate neural network\n",
    "    output = model(X)\n",
    "\n",
    "    # Compute next state\n",
    "    X_prime = compute_next_state(X, output)\n",
    "    \n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_period(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_errors(X, output, X_prime, output_prime):\n",
    "    \"\"\"\n",
    "    Compute error functions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : _type_\n",
    "        _description_\n",
    "    output : _type_\n",
    "        _description_\n",
    "    X_prime : _type_\n",
    "        _description_\n",
    "    output_prime : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    # Compute today's economy\n",
    "    R_orig, R, ph_orig, ph, _, _, h_out_orig, h_out, c_orig, c, A_out, H_out = compute_economy(X, output)\n",
    "\n",
    "    # Compute tomorrow's economy\n",
    "    R_prime_orig, R_prime, ph_prime_orig, ph_prime, _, _, h_prime_out_orig, h_prime_out, c_prime_orig, c_prime, A_prime_out, H_prime_out = compute_economy(X_prime, output_prime)\n",
    "\n",
    "    # Euler equation 1 (Intertemporal)\n",
    "    opt_euler1 = -1 + (c_prime[:, 1:J] / c[:, 0:J-1])*(R*betahat/(Gchi*Pic))**(-1/gamma)\n",
    "    # opt_euler1 = grad_Uc(c[:, 0:J-1], h_out[:, 0:J-1]) - grad_Uc(c_prime[:, 1:J], h_prime_out[:, 1:J]) * R*betahat/(Gchi*Pic)\n",
    "\n",
    "    # Euler equation 2 (Intratemporal)\n",
    "    # opt_euler2 = -1 + (c/h_out)*(((1-alpha)/alpha)*ph*(1+delta-Pih/R))**(-1)\n",
    "    opt_euler_intra = -1 + (c/h_out)*(((1-alpha)/alpha)*ph*(1+delta-Pih/R))**(-1)\n",
    "    opt_euler_intra_prime = -1 + (c_prime/h_prime_out)*(((1-alpha)/alpha)*ph_prime*(1+delta-Pih/R_prime))**(-1)\n",
    "    opt_euler2 = torch.cat([opt_euler_intra, opt_euler_intra_prime], dim=1)\n",
    "\n",
    "    # Punishment for negative consumption\n",
    "    orig_cons = torch.cat([c_orig, c_prime_orig], dim=1)\n",
    "    opt_punish_cons = (1./eps) * torch.maximum(-1 * orig_cons, torch.zeros_like(orig_cons))\n",
    "\n",
    "    # Punishment for negative housing\n",
    "    orig_housing = torch.cat([h_out_orig, h_prime_out_orig], dim=1)\n",
    "    opt_punish_housing = (1./eps) * torch.maximum(-1 * orig_housing, torch.zeros_like(orig_housing))\n",
    "\n",
    "    # Punishment for negative interest rate\n",
    "    orig_R = torch.cat([R_orig, R_prime_orig], dim=1)\n",
    "    opt_punish_R = (1./eps) * torch.maximum(-1 * orig_R, torch.zeros_like(orig_R))\n",
    "\n",
    "    # Punishment for negative house price\n",
    "    orig_ph = torch.cat([ph_orig, ph_prime_orig], dim=1)\n",
    "    opt_punish_ph = (1./eps) * torch.maximum(-1 * orig_ph, torch.zeros_like(orig_ph))\n",
    "\n",
    "    # Market clearing condition for aggregate savings\n",
    "    opt_market_a = A_out\n",
    "    opt_market_a_prime = A_prime_out\n",
    "    opt_market_A = torch.cat([opt_market_a, opt_market_a_prime], dim=1)\n",
    "    # opt_market_A = A_out\n",
    "\n",
    "    # Market clearing condition for aggregate housing\n",
    "    opt_market_h = H_out - Hs\n",
    "    opt_market_h_prime = H_prime_out - Hs\n",
    "    opt_market_H = torch.cat([opt_market_h, opt_market_h_prime], dim=1)\n",
    "    # opt_market_H = H_out - Hs\n",
    "\n",
    "    # Concatenate the equilibrium functions\n",
    "    combined_opt = [opt_euler1, opt_euler2, opt_punish_cons, opt_punish_housing, opt_punish_R, opt_punish_ph, opt_market_A, opt_market_H]\n",
    "    opt_predict = torch.cat(combined_opt, dim=1)\n",
    "\n",
    "    # Define the \"correct\" outputs. For all equilibrium functions, the correct outputs is zero.\n",
    "    opt_correct = torch.zeros_like(opt_predict)\n",
    "\n",
    "    return opt_predict, opt_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(X):\n",
    "    \"\"\"\n",
    "    Simulate and return euler errors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        Euler errors\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate neural network given today's state\n",
    "    output = model(X)\n",
    "\n",
    "    # Compute state tomorrow\n",
    "    X_prime = compute_next_state(X, output)\n",
    "\n",
    "    # Evaluate neural network given tomorrow's state\n",
    "    output_prime = model(X_prime.float())\n",
    "\n",
    "    # Compute euler errors and other equilibrium conditions\n",
    "    opt_predict, opt_correct = euler_errors(X, output, X_prime, output_prime)\n",
    "\n",
    "    return opt_predict, opt_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_predict, opt_correct = simulate(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random economically feasible starting point\n",
    "x_start = torch.rand(size=(1, input_size))\n",
    "\n",
    "num_episodes = 100\n",
    "len_episodes = 16\n",
    "epochs_per_episode = 4\n",
    "minibatch_size = 8\n",
    "num_minibatches = int(len_episodes / minibatch_size)\n",
    "lr = 1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True, capturable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episodes(x_start, episode_length):\n",
    "    \"\"\"Simulate an episode for a given starting point using the current\n",
    "       neural network state.\n",
    "\n",
    "    Args:\n",
    "        x_start: Starting state to simulate forward from,\n",
    "        episode_length: Number of steps to simulate forward,\n",
    "\n",
    "    Returns:\n",
    "        X_episodes: Tensor of states [z, k] to train on (training set).\n",
    "    \"\"\"\n",
    "    \n",
    "    dim_state = torch.Tensor.size(x_start)[1]\n",
    "\n",
    "    # Initialise empty array of episodes\n",
    "    X_episodes = torch.zeros([episode_length, dim_state])\n",
    "    X_episodes[0, :] = x_start\n",
    "    X_old = x_start\n",
    "\n",
    "    for t in range(1, episode_length):\n",
    "        X_new = run_period(X_old.float())\n",
    "        \n",
    "        # Append it to the dataset\n",
    "        X_episodes[t, :] = X_new\n",
    "        X_old = X_new\n",
    "\n",
    "    return X_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate_episodes(x_start, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X_train):\n",
    "\n",
    "    X_train = torch.utils.data.DataLoader(X_train, batch_size=minibatch_size)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs_per_episode):\n",
    "\n",
    "        for X_batch in X_train:\n",
    "\n",
    "            # Forward pass of the model to get Euler errors\n",
    "            opt_predict, opt_correct = simulate(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.mse_loss(opt_predict, opt_correct)             \n",
    "\n",
    "            # Use gradient tape to retrieve gradients of trainable variables with respect to loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(f\"loss: {torch.log(loss):>7f}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_step(x_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=100, suppress=True)\n",
    "\n",
    "def training_algorithm(x_start):\n",
    "\n",
    "    num_episodes = 1000 #try 30000\n",
    "    loss_list = np.empty(num_episodes)\n",
    "\n",
    "    # Training Loops\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Simulate episodes\n",
    "        X_episodes = simulate_episodes(x_start, len_episodes)\n",
    "        X_episodes = torch.Tensor.detach(X_episodes)\n",
    "        \n",
    "        # Train model\n",
    "        loss_value = train_step(X_episodes)\n",
    "\n",
    "        # Update starting episode\n",
    "        x_start = X_episodes[-1, :].reshape([1, -1])\n",
    "\n",
    "        # Store losses, euler errors\n",
    "        loss_list[episode] = torch.log(loss_value)\n",
    "        \n",
    "        # Log\n",
    "        if episode % 1 == 0:\n",
    "            print(f\"Episode {episode}: log10(loss): {torch.log(loss_value):.5f}\")\n",
    "            print(torch.Tensor.cpu(x_start).numpy())\n",
    "          \n",
    "    return x_start, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: log10(loss): 1.53755\n",
      "[[ 0.0012  0.1099  0.8222  1.2334  0.2034  0.     -0.079   0.1346  0.      0.001   0.0238]]\n",
      "Episode 1: log10(loss): 2.42283\n",
      "[[ 0.2883  0.3892  0.8222  1.3642  0.2969  0.     -0.1429  0.0104  0.      0.3675  0.1924]]\n",
      "Episode 2: log10(loss): 2.74060\n",
      "[[ 0.337   0.6083  0.8222  1.4802  0.4563  0.     -0.0243 -0.135   0.      0.4012  0.3582]]\n",
      "Episode 3: log10(loss): 2.65693\n",
      "[[ 0.3638  0.7126  0.8222  1.5244  0.5779  0.      0.0609 -0.2356  0.      0.3872  0.4606]]\n",
      "Episode 4: log10(loss): 2.33198\n",
      "[[ 0.3971  0.721   0.8222  1.5243  0.6108  0.      0.0974 -0.2939  0.      0.374   0.507 ]]\n",
      "Episode 5: log10(loss): 1.95598\n",
      "[[ 0.4331  0.6781  0.8222  1.5056  0.5835  0.      0.1011 -0.3228  0.      0.368   0.5202]]\n",
      "Episode 6: log10(loss): 1.63024\n",
      "[[ 0.464   0.6215  0.8222  1.4826  0.5377  0.      0.0907 -0.3346  0.      0.3663  0.519 ]]\n",
      "Episode 7: log10(loss): 1.37430\n",
      "[[ 0.4875  0.5692  0.8222  1.4612  0.4947  0.      0.0769 -0.3375  0.      0.3664  0.5134]]\n",
      "Episode 8: log10(loss): 1.17662\n",
      "[[ 0.5046  0.5258  0.8222  1.4432  0.4596  0.      0.0639 -0.3363  0.      0.3669  0.5072]]\n",
      "Episode 9: log10(loss): 1.02120\n",
      "[[ 0.5171  0.4904  0.8222  1.4284  0.432   0.      0.0526 -0.3333  0.      0.3676  0.5016]]\n",
      "Episode 10: log10(loss): 0.89522\n",
      "[[ 0.5265  0.4613  0.8222  1.416   0.41    0.      0.0429 -0.3295  0.      0.3683  0.4968]]\n",
      "Episode 11: log10(loss): 0.78987\n",
      "[[ 0.5338  0.4367  0.8222  1.4054  0.392   0.      0.0346 -0.3254  0.      0.3689  0.4926]]\n",
      "Episode 12: log10(loss): 0.69943\n",
      "[[ 0.5398  0.4154  0.8222  1.3962  0.3769  0.      0.0272 -0.3212  0.      0.3695  0.489 ]]\n",
      "Episode 13: log10(loss): 0.62019\n",
      "[[ 0.5448  0.3965  0.8222  1.3879  0.3638  0.      0.0206 -0.3171  0.      0.37    0.4858]]\n",
      "Episode 14: log10(loss): 0.54976\n",
      "[[ 0.5492  0.3795  0.8222  1.3804  0.3523  0.      0.0146 -0.313   0.      0.3705  0.483 ]]\n",
      "Episode 15: log10(loss): 0.48651\n",
      "[[ 0.553   0.3639  0.8222  1.3735  0.342   0.      0.0091 -0.3089  0.      0.3709  0.4804]]\n",
      "Episode 16: log10(loss): 0.42934\n",
      "[[ 0.5564  0.3495  0.8222  1.3671  0.3327  0.      0.0039 -0.3048  0.      0.3713  0.4781]]\n",
      "Episode 17: log10(loss): 0.37743\n",
      "[[ 0.5594  0.3362  0.8222  1.3611  0.3243  0.     -0.001  -0.3009  0.      0.3717  0.4759]]\n",
      "Episode 18: log10(loss): 0.33015\n",
      "[[ 0.5622  0.3238  0.8222  1.3555  0.3166  0.     -0.0056 -0.2969  0.      0.372   0.4739]]\n",
      "Episode 19: log10(loss): 0.28701\n",
      "[[ 0.5647  0.3121  0.8222  1.3503  0.3095  0.     -0.0099 -0.293   0.      0.3724  0.472 ]]\n",
      "Episode 20: log10(loss): 0.24761\n",
      "[[ 0.567   0.3013  0.8222  1.3453  0.303   0.     -0.014  -0.2892  0.      0.3727  0.4703]]\n",
      "Episode 21: log10(loss): 0.21162\n",
      "[[ 0.5691  0.291   0.8222  1.3406  0.297   0.     -0.0179 -0.2854  0.      0.373   0.4687]]\n",
      "Episode 22: log10(loss): 0.17874\n",
      "[[ 0.5711  0.2814  0.8222  1.3362  0.2914  0.     -0.0217 -0.2816  0.      0.3733  0.4672]]\n",
      "Episode 23: log10(loss): 0.14872\n",
      "[[ 0.5729  0.2724  0.8222  1.332   0.2863  0.     -0.0253 -0.2779  0.      0.3736  0.4657]]\n",
      "Episode 24: log10(loss): 0.12133\n",
      "[[ 0.5746  0.2639  0.8222  1.3281  0.2816  0.     -0.0288 -0.2742  0.      0.3739  0.4644]]\n",
      "Episode 25: log10(loss): 0.09637\n",
      "[[ 0.5762  0.2559  0.8222  1.3244  0.2773  0.     -0.0321 -0.2705  0.      0.3742  0.4631]]\n",
      "Episode 26: log10(loss): 0.07363\n",
      "[[ 0.5776  0.2484  0.8222  1.3209  0.2733  0.     -0.0353 -0.2669  0.      0.3744  0.462 ]]\n",
      "Episode 27: log10(loss): 0.05295\n",
      "[[ 0.579   0.2414  0.8222  1.3175  0.2696  0.     -0.0384 -0.2633  0.      0.3747  0.4609]]\n",
      "Episode 28: log10(loss): 0.03415\n",
      "[[ 0.5803  0.2348  0.8222  1.3144  0.2663  0.     -0.0414 -0.2597  0.      0.375   0.4598]]\n",
      "Episode 29: log10(loss): 0.01708\n",
      "[[ 0.5814  0.2287  0.8222  1.3114  0.2632  0.     -0.0442 -0.2562  0.      0.3752  0.4589]]\n",
      "Episode 30: log10(loss): 0.00159\n",
      "[[ 0.5826  0.223   0.8222  1.3087  0.2604  0.     -0.047  -0.2527  0.      0.3755  0.458 ]]\n",
      "Episode 31: log10(loss): -0.01248\n",
      "[[ 0.5836  0.2176  0.8222  1.3061  0.2579  0.     -0.0497 -0.2493  0.      0.3758  0.4571]]\n",
      "Episode 32: log10(loss): -0.02524\n",
      "[[ 0.5846  0.2127  0.8222  1.3036  0.2557  0.     -0.0523 -0.2459  0.      0.376   0.4563]]\n",
      "Episode 33: log10(loss): -0.03685\n",
      "[[ 0.5856  0.2081  0.8222  1.3013  0.2536  0.     -0.0549 -0.2425  0.      0.3763  0.4556]]\n",
      "Episode 34: log10(loss): -0.04740\n",
      "[[ 0.5865  0.2039  0.8222  1.2992  0.2518  0.     -0.0573 -0.2391  0.      0.3765  0.4549]]\n",
      "Episode 35: log10(loss): -0.05704\n",
      "[[ 0.5873  0.2     0.8222  1.2972  0.2502  0.     -0.0597 -0.2358  0.      0.3768  0.4543]]\n",
      "Episode 36: log10(loss): -0.06585\n",
      "[[ 0.5882  0.1964  0.8222  1.2953  0.2489  0.     -0.062  -0.2325  0.      0.3771  0.4537]]\n",
      "Episode 37: log10(loss): -0.07395\n",
      "[[ 0.589   0.1932  0.8222  1.2936  0.2477  0.     -0.0643 -0.2292  0.      0.3773  0.4532]]\n",
      "Episode 38: log10(loss): -0.08143\n",
      "[[ 0.5897  0.1902  0.8222  1.292   0.2467  0.     -0.0665 -0.226   0.      0.3776  0.4527]]\n",
      "Episode 39: log10(loss): -0.08837\n",
      "[[ 0.5904  0.1876  0.8222  1.2905  0.2459  0.     -0.0686 -0.2228  0.      0.3778  0.4523]]\n",
      "Episode 40: log10(loss): -0.09484\n",
      "[[ 0.5911  0.1852  0.8222  1.2891  0.2452  0.     -0.0707 -0.2196  0.      0.3781  0.4519]]\n",
      "Episode 41: log10(loss): -0.10092\n",
      "[[ 0.5918  0.183   0.8222  1.2879  0.2447  0.     -0.0728 -0.2164  0.      0.3784  0.4515]]\n",
      "Episode 42: log10(loss): -0.10667\n",
      "[[ 0.5925  0.1811  0.8222  1.2867  0.2443  0.     -0.0748 -0.2132  0.      0.3786  0.4512]]\n",
      "Episode 43: log10(loss): -0.11213\n",
      "[[ 0.5931  0.1794  0.8222  1.2856  0.2441  0.     -0.0768 -0.2101  0.      0.3789  0.4509]]\n",
      "Episode 44: log10(loss): -0.11735\n",
      "[[ 0.5938  0.1779  0.8222  1.2846  0.244   0.     -0.0787 -0.207   0.      0.3792  0.4506]]\n",
      "Episode 45: log10(loss): -0.12237\n",
      "[[ 0.5944  0.1766  0.8222  1.2837  0.244   0.     -0.0806 -0.2039  0.      0.3794  0.4504]]\n",
      "Episode 46: log10(loss): -0.12721\n",
      "[[ 0.595   0.1754  0.8222  1.2829  0.2441  0.     -0.0824 -0.2009  0.      0.3797  0.4501]]\n",
      "Episode 47: log10(loss): -0.13191\n",
      "[[ 0.5956  0.1744  0.8222  1.2821  0.2443  0.     -0.0843 -0.1978  0.      0.38    0.45  ]]\n",
      "Episode 48: log10(loss): -0.13648\n",
      "[[ 0.5962  0.1736  0.8222  1.2814  0.2445  0.     -0.0861 -0.1948  0.      0.3803  0.4498]]\n",
      "Episode 49: log10(loss): -0.14095\n",
      "[[ 0.5967  0.1728  0.8222  1.2807  0.2449  0.     -0.0878 -0.1918  0.      0.3805  0.4496]]\n",
      "Episode 50: log10(loss): -0.14531\n",
      "[[ 0.5973  0.1722  0.8222  1.2801  0.2453  0.     -0.0896 -0.1888  0.      0.3808  0.4495]]\n",
      "Episode 51: log10(loss): -0.14960\n",
      "[[ 0.5979  0.1717  0.8222  1.2796  0.2457  0.     -0.0913 -0.1858  0.      0.3811  0.4494]]\n",
      "Episode 52: log10(loss): -0.15380\n",
      "[[ 0.5984  0.1712  0.8222  1.279   0.2462  0.     -0.093  -0.1828  0.      0.3814  0.4493]]\n",
      "Episode 53: log10(loss): -0.15793\n",
      "[[ 0.599   0.1708  0.8222  1.2785  0.2468  0.     -0.0947 -0.1799  0.      0.3816  0.4492]]\n",
      "Episode 54: log10(loss): -0.16200\n",
      "[[ 0.5995  0.1705  0.8222  1.278   0.2474  0.     -0.0964 -0.1769  0.      0.3819  0.4491]]\n",
      "Episode 55: log10(loss): -0.16601\n",
      "[[ 0.6001  0.1703  0.8222  1.2776  0.248   0.     -0.098  -0.174   0.      0.3822  0.449 ]]\n",
      "Episode 56: log10(loss): -0.16996\n",
      "[[ 0.6006  0.1701  0.8222  1.2772  0.2487  0.     -0.0997 -0.1711  0.      0.3825  0.4489]]\n",
      "Episode 57: log10(loss): -0.17385\n",
      "[[ 0.6012  0.1699  0.8222  1.2768  0.2493  0.     -0.1013 -0.1682  0.      0.3827  0.4489]]\n",
      "Episode 58: log10(loss): -0.17769\n",
      "[[ 0.6017  0.1698  0.8222  1.2764  0.25    0.     -0.1029 -0.1654  0.      0.383   0.4488]]\n",
      "Episode 59: log10(loss): -0.18148\n",
      "[[ 0.6022  0.1697  0.8222  1.276   0.2507  0.     -0.1045 -0.1625  0.      0.3833  0.4488]]\n",
      "Episode 60: log10(loss): -0.18521\n",
      "[[ 0.6028  0.1696  0.8222  1.2756  0.2515  0.     -0.1061 -0.1596  0.      0.3836  0.4487]]\n",
      "Episode 61: log10(loss): -0.18889\n",
      "[[ 0.6033  0.1696  0.8222  1.2753  0.2522  0.     -0.1076 -0.1568  0.      0.3838  0.4487]]\n",
      "Episode 62: log10(loss): -0.19252\n",
      "[[ 0.6038  0.1696  0.8222  1.2749  0.253   0.     -0.1092 -0.154   0.      0.3841  0.4487]]\n",
      "Episode 63: log10(loss): -0.19610\n",
      "[[ 0.6044  0.1696  0.8222  1.2746  0.2537  0.     -0.1107 -0.1512  0.      0.3844  0.4487]]\n",
      "Episode 64: log10(loss): -0.19963\n",
      "[[ 0.6049  0.1696  0.8222  1.2743  0.2545  0.     -0.1123 -0.1484  0.      0.3846  0.4486]]\n",
      "Episode 65: log10(loss): -0.20312\n",
      "[[ 0.6054  0.1696  0.8222  1.274   0.2552  0.     -0.1138 -0.1456  0.      0.3849  0.4486]]\n",
      "Episode 66: log10(loss): -0.20655\n",
      "[[ 0.6059  0.1696  0.8222  1.2737  0.256   0.     -0.1153 -0.1429  0.      0.3852  0.4486]]\n",
      "Episode 67: log10(loss): -0.20994\n",
      "[[ 0.6064  0.1696  0.8222  1.2733  0.2568  0.     -0.1168 -0.1401  0.      0.3854  0.4486]]\n",
      "Episode 68: log10(loss): -0.21327\n",
      "[[ 0.607   0.1696  0.8222  1.273   0.2575  0.     -0.1183 -0.1374  0.      0.3857  0.4486]]\n",
      "Episode 69: log10(loss): -0.21656\n",
      "[[ 0.6075  0.1697  0.8222  1.2727  0.2583  0.     -0.1197 -0.1347  0.      0.386   0.4486]]\n",
      "Episode 70: log10(loss): -0.21980\n",
      "[[ 0.608   0.1697  0.8222  1.2724  0.259   0.     -0.1212 -0.132   0.      0.3862  0.4486]]\n",
      "Episode 71: log10(loss): -0.22300\n",
      "[[ 0.6085  0.1698  0.8222  1.2721  0.2598  0.     -0.1226 -0.1293  0.      0.3865  0.4486]]\n",
      "Episode 72: log10(loss): -0.22615\n",
      "[[ 0.609   0.1698  0.8222  1.2718  0.2606  0.     -0.1241 -0.1267  0.      0.3868  0.4486]]\n",
      "Episode 73: log10(loss): -0.22925\n",
      "[[ 0.6096  0.1699  0.8222  1.2715  0.2613  0.     -0.1255 -0.124   0.      0.387   0.4486]]\n",
      "Episode 74: log10(loss): -0.23231\n",
      "[[ 0.6101  0.1699  0.8222  1.2713  0.2621  0.     -0.1269 -0.1214  0.      0.3873  0.4486]]\n",
      "Episode 75: log10(loss): -0.23532\n",
      "[[ 0.6106  0.1699  0.8222  1.271   0.2628  0.     -0.1283 -0.1188  0.      0.3876  0.4486]]\n",
      "Episode 76: log10(loss): -0.23829\n",
      "[[ 0.6111  0.17    0.8222  1.2707  0.2636  0.     -0.1297 -0.1162  0.      0.3878  0.4486]]\n",
      "Episode 77: log10(loss): -0.24121\n",
      "[[ 0.6116  0.17    0.8222  1.2704  0.2643  0.     -0.1311 -0.1136  0.      0.3881  0.4486]]\n",
      "Episode 78: log10(loss): -0.24409\n",
      "[[ 0.6121  0.1701  0.8222  1.2701  0.2651  0.     -0.1325 -0.111   0.      0.3883  0.4486]]\n",
      "Episode 79: log10(loss): -0.24692\n",
      "[[ 0.6126  0.1701  0.8222  1.2698  0.2658  0.     -0.1338 -0.1085  0.      0.3886  0.4486]]\n",
      "Episode 80: log10(loss): -0.24971\n",
      "[[ 0.6131  0.1701  0.8222  1.2695  0.2665  0.     -0.1352 -0.106   0.      0.3888  0.4487]]\n",
      "Episode 81: log10(loss): -0.25246\n",
      "[[ 0.6136  0.1702  0.8222  1.2693  0.2673  0.     -0.1365 -0.1035  0.      0.3891  0.4487]]\n",
      "Episode 82: log10(loss): -0.25517\n",
      "[[ 0.6142  0.1702  0.8222  1.269   0.268   0.     -0.1379 -0.101   0.      0.3893  0.4487]]\n",
      "Episode 83: log10(loss): -0.25783\n",
      "[[ 0.6147  0.1703  0.8222  1.2687  0.2687  0.     -0.1392 -0.0985  0.      0.3896  0.4487]]\n",
      "Episode 84: log10(loss): -0.26045\n",
      "[[ 0.6152  0.1703  0.8222  1.2684  0.2694  0.     -0.1405 -0.096   0.      0.3898  0.4487]]\n",
      "Episode 85: log10(loss): -0.26303\n",
      "[[ 0.6157  0.1703  0.8222  1.2681  0.2702  0.     -0.1418 -0.0936  0.      0.3901  0.4488]]\n",
      "Episode 86: log10(loss): -0.26557\n",
      "[[ 0.6162  0.1703  0.8222  1.2679  0.2709  0.     -0.1431 -0.0912  0.      0.3903  0.4488]]\n",
      "Episode 87: log10(loss): -0.26807\n",
      "[[ 0.6167  0.1704  0.8222  1.2676  0.2716  0.     -0.1444 -0.0888  0.      0.3906  0.4488]]\n",
      "Episode 88: log10(loss): -0.27053\n",
      "[[ 0.6172  0.1704  0.8222  1.2673  0.2723  0.     -0.1457 -0.0864  0.      0.3908  0.4488]]\n",
      "Episode 89: log10(loss): -0.27296\n",
      "[[ 0.6177  0.1704  0.8222  1.2671  0.273   0.     -0.1469 -0.084   0.      0.391   0.4489]]\n",
      "Episode 90: log10(loss): -0.27534\n",
      "[[ 0.6182  0.1704  0.8222  1.2668  0.2736  0.     -0.1482 -0.0817  0.      0.3913  0.4489]]\n",
      "Episode 91: log10(loss): -0.27768\n",
      "[[ 0.6186  0.1705  0.8222  1.2665  0.2743  0.     -0.1494 -0.0793  0.      0.3915  0.4489]]\n",
      "Episode 92: log10(loss): -0.27999\n",
      "[[ 0.6191  0.1705  0.8222  1.2662  0.275   0.     -0.1506 -0.077   0.      0.3918  0.449 ]]\n",
      "Episode 93: log10(loss): -0.28226\n",
      "[[ 0.6196  0.1705  0.8222  1.266   0.2757  0.     -0.1519 -0.0747  0.      0.392   0.449 ]]\n",
      "Episode 94: log10(loss): -0.28449\n",
      "[[ 0.6201  0.1705  0.8222  1.2657  0.2764  0.     -0.1531 -0.0724  0.      0.3922  0.4491]]\n",
      "Episode 95: log10(loss): -0.28669\n",
      "[[ 0.6206  0.1705  0.8222  1.2654  0.277   0.     -0.1543 -0.0702  0.      0.3925  0.4491]]\n",
      "Episode 96: log10(loss): -0.28885\n",
      "[[ 0.6211  0.1705  0.8222  1.2652  0.2777  0.     -0.1555 -0.0679  0.      0.3927  0.4491]]\n",
      "Episode 97: log10(loss): -0.29098\n",
      "[[ 0.6216  0.1706  0.8222  1.2649  0.2783  0.     -0.1567 -0.0657  0.      0.3929  0.4492]]\n",
      "Episode 98: log10(loss): -0.29307\n",
      "[[ 0.6221  0.1706  0.8222  1.2646  0.279   0.     -0.1578 -0.0635  0.      0.3931  0.4492]]\n",
      "Episode 99: log10(loss): -0.29513\n",
      "[[ 0.6225  0.1706  0.8222  1.2644  0.2796  0.     -0.159  -0.0613  0.      0.3934  0.4493]]\n",
      "Episode 100: log10(loss): -0.29715\n",
      "[[ 0.623   0.1706  0.8222  1.2641  0.2803  0.     -0.1602 -0.0591  0.      0.3936  0.4493]]\n",
      "Episode 101: log10(loss): -0.29914\n",
      "[[ 0.6235  0.1706  0.8222  1.2639  0.2809  0.     -0.1613 -0.057   0.      0.3938  0.4494]]\n",
      "Episode 102: log10(loss): -0.30110\n",
      "[[ 0.624   0.1706  0.8222  1.2636  0.2815  0.     -0.1624 -0.0548  0.      0.394   0.4494]]\n",
      "Episode 103: log10(loss): -0.30302\n",
      "[[ 0.6245  0.1706  0.8222  1.2634  0.2822  0.     -0.1636 -0.0527  0.      0.3943  0.4495]]\n",
      "Episode 104: log10(loss): -0.30491\n",
      "[[ 0.6249  0.1706  0.8222  1.2631  0.2828  0.     -0.1647 -0.0506  0.      0.3945  0.4495]]\n",
      "Episode 105: log10(loss): -0.30678\n",
      "[[ 0.6254  0.1706  0.8222  1.2628  0.2834  0.     -0.1658 -0.0485  0.      0.3947  0.4496]]\n",
      "Episode 106: log10(loss): -0.30861\n",
      "[[ 0.6259  0.1706  0.8222  1.2626  0.284   0.     -0.1669 -0.0465  0.      0.3949  0.4496]]\n",
      "Episode 107: log10(loss): -0.31040\n",
      "[[ 0.6264  0.1706  0.8222  1.2623  0.2846  0.     -0.168  -0.0444  0.      0.3951  0.4497]]\n",
      "Episode 108: log10(loss): -0.31217\n",
      "[[ 0.6268  0.1706  0.8222  1.2621  0.2853  0.     -0.1691 -0.0424  0.      0.3953  0.4498]]\n",
      "Episode 109: log10(loss): -0.31391\n",
      "[[ 0.6273  0.1706  0.8222  1.2618  0.2859  0.     -0.1702 -0.0404  0.      0.3956  0.4498]]\n",
      "Episode 110: log10(loss): -0.31562\n",
      "[[ 0.6278  0.1706  0.8222  1.2616  0.2865  0.     -0.1712 -0.0384  0.      0.3958  0.4499]]\n",
      "Episode 111: log10(loss): -0.31731\n",
      "[[ 0.6282  0.1706  0.8222  1.2613  0.287   0.     -0.1723 -0.0364  0.      0.396   0.4499]]\n",
      "Episode 112: log10(loss): -0.31896\n",
      "[[ 0.6287  0.1706  0.8222  1.2611  0.2876  0.     -0.1733 -0.0344  0.      0.3962  0.45  ]]\n",
      "Episode 113: log10(loss): -0.32058\n",
      "[[ 0.6292  0.1706  0.8222  1.2608  0.2882  0.     -0.1744 -0.0325  0.      0.3964  0.4501]]\n",
      "Episode 114: log10(loss): -0.32218\n",
      "[[ 0.6296  0.1706  0.8222  1.2606  0.2888  0.     -0.1754 -0.0306  0.      0.3966  0.4501]]\n",
      "Episode 115: log10(loss): -0.32375\n",
      "[[ 0.6301  0.1706  0.8222  1.2604  0.2894  0.     -0.1764 -0.0287  0.      0.3968  0.4502]]\n",
      "Episode 116: log10(loss): -0.32530\n",
      "[[ 0.6305  0.1706  0.8222  1.2601  0.2899  0.     -0.1774 -0.0268  0.      0.397   0.4503]]\n",
      "Episode 117: log10(loss): -0.32682\n",
      "[[ 0.631   0.1706  0.8222  1.2599  0.2905  0.     -0.1785 -0.0249  0.      0.3972  0.4503]]\n",
      "Episode 118: log10(loss): -0.32831\n",
      "[[ 0.6314  0.1706  0.8222  1.2596  0.2911  0.     -0.1795 -0.023   0.      0.3974  0.4504]]\n",
      "Episode 119: log10(loss): -0.32977\n",
      "[[ 0.6319  0.1706  0.8222  1.2594  0.2916  0.     -0.1805 -0.0212  0.      0.3976  0.4505]]\n",
      "Episode 120: log10(loss): -0.33122\n",
      "[[ 0.6324  0.1706  0.8222  1.2592  0.2922  0.     -0.1814 -0.0194  0.      0.3978  0.4506]]\n",
      "Episode 121: log10(loss): -0.33263\n",
      "[[ 0.6328  0.1706  0.8222  1.2589  0.2927  0.     -0.1824 -0.0176  0.      0.398   0.4506]]\n",
      "Episode 122: log10(loss): -0.33403\n",
      "[[ 0.6333  0.1706  0.8222  1.2587  0.2933  0.     -0.1834 -0.0158  0.      0.3982  0.4507]]\n",
      "Episode 123: log10(loss): -0.33540\n",
      "[[ 0.6337  0.1706  0.8222  1.2585  0.2938  0.     -0.1843 -0.014   0.      0.3984  0.4508]]\n",
      "Episode 124: log10(loss): -0.33674\n",
      "[[ 0.6341  0.1706  0.8222  1.2582  0.2944  0.     -0.1853 -0.0122  0.      0.3986  0.4509]]\n",
      "Episode 125: log10(loss): -0.33806\n",
      "[[ 0.6346  0.1705  0.8222  1.258   0.2949  0.     -0.1862 -0.0105  0.      0.3988  0.451 ]]\n",
      "Episode 126: log10(loss): -0.33936\n",
      "[[ 0.635   0.1705  0.8222  1.2578  0.2954  0.     -0.1872 -0.0088  0.      0.399   0.451 ]]\n",
      "Episode 127: log10(loss): -0.34064\n",
      "[[ 0.6355  0.1705  0.8222  1.2575  0.2959  0.     -0.1881 -0.0071  0.      0.3991  0.4511]]\n",
      "Episode 128: log10(loss): -0.34190\n",
      "[[ 0.6359  0.1705  0.8222  1.2573  0.2965  0.     -0.189  -0.0054  0.      0.3993  0.4512]]\n",
      "Episode 129: log10(loss): -0.34313\n",
      "[[ 0.6364  0.1705  0.8222  1.2571  0.297   0.     -0.19   -0.0037  0.      0.3995  0.4513]]\n",
      "Episode 130: log10(loss): -0.34435\n",
      "[[ 0.6368  0.1705  0.8222  1.2569  0.2975  0.     -0.1909 -0.0021  0.      0.3997  0.4514]]\n",
      "Episode 131: log10(loss): -0.34554\n",
      "[[ 0.6372  0.1705  0.8222  1.2567  0.298   0.     -0.1918 -0.0004  0.      0.3999  0.4515]]\n",
      "Episode 132: log10(loss): -0.34671\n",
      "[[ 0.6377  0.1705  0.8222  1.2564  0.2985  0.     -0.1927  0.0012  0.      0.4001  0.4515]]\n",
      "Episode 133: log10(loss): -0.34786\n",
      "[[ 0.6381  0.1705  0.8222  1.2562  0.299   0.     -0.1936  0.0028  0.      0.4002  0.4516]]\n",
      "Episode 134: log10(loss): -0.34899\n",
      "[[ 0.6385  0.1705  0.8222  1.256   0.2995  0.     -0.1944  0.0044  0.      0.4004  0.4517]]\n",
      "Episode 135: log10(loss): -0.35011\n",
      "[[ 0.639   0.1705  0.8222  1.2558  0.3     0.     -0.1953  0.006   0.      0.4006  0.4518]]\n",
      "Episode 136: log10(loss): -0.35120\n",
      "[[ 0.6394  0.1704  0.8222  1.2556  0.3005  0.     -0.1962  0.0076  0.      0.4008  0.4519]]\n",
      "Episode 137: log10(loss): -0.35227\n",
      "[[ 0.6398  0.1704  0.8222  1.2553  0.301   0.     -0.197   0.0091  0.      0.401   0.452 ]]\n",
      "Episode 138: log10(loss): -0.35333\n",
      "[[ 0.6402  0.1704  0.8222  1.2551  0.3015  0.     -0.1979  0.0106  0.      0.4011  0.4521]]\n",
      "Episode 139: log10(loss): -0.35437\n",
      "[[ 0.6407  0.1704  0.8222  1.2549  0.3019  0.     -0.1987  0.0121  0.      0.4013  0.4522]]\n",
      "Episode 140: log10(loss): -0.35539\n",
      "[[ 0.6411  0.1704  0.8222  1.2547  0.3024  0.     -0.1996  0.0136  0.      0.4015  0.4523]]\n",
      "Episode 141: log10(loss): -0.35639\n",
      "[[ 0.6415  0.1704  0.8222  1.2545  0.3029  0.     -0.2004  0.0151  0.      0.4016  0.4524]]\n",
      "Episode 142: log10(loss): -0.35737\n",
      "[[ 0.6419  0.1704  0.8222  1.2543  0.3034  0.     -0.2012  0.0166  0.      0.4018  0.4525]]\n",
      "Episode 143: log10(loss): -0.35834\n",
      "[[ 0.6423  0.1704  0.8222  1.2541  0.3038  0.     -0.2021  0.0181  0.      0.402   0.4526]]\n",
      "Episode 144: log10(loss): -0.35929\n",
      "[[ 0.6428  0.1704  0.8222  1.2539  0.3043  0.     -0.2029  0.0195  0.      0.4021  0.4527]]\n",
      "Episode 145: log10(loss): -0.36023\n",
      "[[ 0.6432  0.1704  0.8222  1.2537  0.3047  0.     -0.2037  0.0209  0.      0.4023  0.4528]]\n",
      "Episode 146: log10(loss): -0.36115\n",
      "[[ 0.6436  0.1704  0.8222  1.2535  0.3052  0.     -0.2045  0.0223  0.      0.4025  0.4529]]\n",
      "Episode 147: log10(loss): -0.36205\n",
      "[[ 0.644   0.1704  0.8222  1.2533  0.3056  0.     -0.2053  0.0237  0.      0.4026  0.453 ]]\n",
      "Episode 148: log10(loss): -0.36294\n",
      "[[ 0.6444  0.1704  0.8222  1.2531  0.3061  0.     -0.2061  0.0251  0.      0.4028  0.4531]]\n",
      "Episode 149: log10(loss): -0.36381\n",
      "[[ 0.6448  0.1703  0.8222  1.2529  0.3065  0.     -0.2069  0.0265  0.      0.403   0.4532]]\n",
      "Episode 150: log10(loss): -0.36467\n",
      "[[ 0.6452  0.1703  0.8222  1.2527  0.307   0.     -0.2076  0.0278  0.      0.4031  0.4533]]\n",
      "Episode 151: log10(loss): -0.36551\n",
      "[[ 0.6456  0.1703  0.8222  1.2525  0.3074  0.     -0.2084  0.0291  0.      0.4033  0.4534]]\n",
      "Episode 152: log10(loss): -0.36634\n",
      "[[ 0.646   0.1703  0.8222  1.2523  0.3078  0.     -0.2092  0.0305  0.      0.4034  0.4535]]\n",
      "Episode 153: log10(loss): -0.36715\n",
      "[[ 0.6464  0.1703  0.8222  1.2521  0.3082  0.     -0.2099  0.0318  0.      0.4036  0.4536]]\n",
      "Episode 154: log10(loss): -0.36796\n",
      "[[ 0.6469  0.1703  0.8222  1.2519  0.3087  0.     -0.2107  0.0331  0.      0.4037  0.4537]]\n",
      "Episode 155: log10(loss): -0.36874\n",
      "[[ 0.6473  0.1703  0.8222  1.2517  0.3091  0.     -0.2114  0.0344  0.      0.4039  0.4539]]\n",
      "Episode 156: log10(loss): -0.36952\n",
      "[[ 0.6477  0.1703  0.8222  1.2515  0.3095  0.     -0.2122  0.0356  0.      0.404   0.454 ]]\n",
      "Episode 157: log10(loss): -0.37028\n",
      "[[ 0.6481  0.1703  0.8222  1.2513  0.3099  0.     -0.2129  0.0369  0.      0.4042  0.4541]]\n",
      "Episode 158: log10(loss): -0.37102\n",
      "[[ 0.6484  0.1703  0.8222  1.2511  0.3103  0.     -0.2137  0.0381  0.      0.4043  0.4542]]\n",
      "Episode 159: log10(loss): -0.37176\n",
      "[[ 0.6488  0.1703  0.8222  1.2509  0.3107  0.     -0.2144  0.0393  0.      0.4045  0.4543]]\n"
     ]
    }
   ],
   "source": [
    "x_final, loss_list = training_algorithm(x_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4853306a52d5055eb45ccebd398f44d73d5814f856fe5205a72cd2003d3ca148"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
