{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else cpu)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further setting up\n",
    "\n",
    "# Set the seed for replicable results\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Helper variable\n",
    "lb = 1e-3 # Define lower bound for c, h\n",
    "eps = 1e-5 # Define inverse punishment value for negative predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SCALAR PARAMETERS\n",
    "alpha = 0.1\n",
    "beta = 0.99**25\n",
    "gamma = 3\n",
    "J = 3\n",
    "jret = 2\n",
    "thetaret = 0.2\n",
    "delta = (1 + 0.015)**25 - 1\n",
    "Gmu = (1 + 0.005)**25\n",
    "Gchi = (1 + 0.02)**25\n",
    "Pic = (1 + 0.02)**25\n",
    "Hs = 0.79/5\n",
    "\n",
    "# COMPUTE PARAMETER TRANSFORMATIONS\n",
    "betahat = Gchi**((1-alpha)*(1-gamma))*beta\n",
    "Pih = Gchi * Pic\n",
    "\n",
    "# DEFINE AGE-VARYING PRODUCTIVITY AND SURVIVAL RATES\n",
    "chi1 = 0.874264\n",
    "mu1 = 0.493497\n",
    "\n",
    "chi = chi1 * np.array([1, 1.5])\n",
    "\n",
    "zeta = np.array([0.976163, 0.784336, 0])\n",
    "zeta_0 = np.catenate(([1], zeta[0:2]))\n",
    "zeta_0 = torch.from_numpy(zeta_0).to(device)\n",
    "\n",
    "# COMPUTE POPULATION DISTRIBUTION\n",
    "mu = np.zeros(J)\n",
    "mu[0] = mu1\n",
    "for j in range(1, J):\n",
    "    mu[j] = (zeta[j-1] / Gmu) * mu[j-1]\n",
    "\n",
    "# COMPUTE TAX RATE\n",
    "tau = np.sum([thetaret*Gchi**(jret - j)*mu[j] for j in range(jret, J)]) / np.sum(mu[0:jret]*chi)\n",
    "\n",
    "# COMPUTE AGE-VARYING INCOME\n",
    "y = torch.zeros(J)\n",
    "for j in range(J):\n",
    "    if j < jret:\n",
    "        y[j] = (1-tau) * chi[j]\n",
    "    else:\n",
    "        y[j] = thetaret * Gchi ** (jret-j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "len_episodes = 128\n",
    "epochs_per_episode = 20\n",
    "minibatch_size = 32\n",
    "num_minibatches = int(len_episodes / minibatch_size)\n",
    "lr = 1e-5\n",
    "\n",
    "#Â Neural network architecture parameters\n",
    "input_size = 2 + 3*J  # Dimension of extended state space (4 aggregate quantities and 3 distributions)\n",
    "output_size = J + J-1 + 2  # Output dimension: 3 for housing, 2 for assets, 2 aggregate quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, output_size)\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "model = NeuralNetwork(input_size, output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(1, input_size))\n",
    "# X.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.function\n",
    "def unpack_output(output):\n",
    "    \"\"\"\n",
    "    Unpacks the output of the neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : tensor\n",
    "        Output of neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack simulated housing and financial assets\n",
    "    R_orig = torch.unsqueeze(output[:, 0], 1)\n",
    "    ph_orig = torch.unsqueeze(output[:, 1], 1)\n",
    "    a_out = output[:, 2:4]\n",
    "    h_out_orig = output[:, 4:7]\n",
    "\n",
    "    return R_orig, ph_orig, a_out, h_out_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1016]], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[0.0922]], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[0.0256, 0.0772]], grad_fn=<SliceBackward0>),\n",
       " tensor([[-0.0525,  0.0330,  0.0906]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_output(model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_economy(X, output):\n",
    "    \"\"\"\n",
    "    Solves today's economy given state and output of neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack inputs and outputs\n",
    "    w = X[:, 2:5] # Distribution of tangible wealth\n",
    "    R_orig, ph_orig, a_out, h_out_orig = unpack_output(output)\n",
    "\n",
    "    # The network can predict negative values before it learns not to. \n",
    "    R = torch.maximum(R_orig, torch.ones_like(R_orig)*eps)\n",
    "    ph = torch.maximum(ph_orig, torch.ones_like(ph_orig)*eps)\n",
    "    h_out = torch.maximum(h_out_orig, torch.ones_like(h_out_orig)*lb)\n",
    "\n",
    "    # Compute savings for oldest cohort\n",
    "    a_out_J = -(Pih * ph * torch.unsqueeze(h_out[:, J-1], 1)) / R\n",
    "    a_out_all = torch.cat([a_out, a_out_J], dim=1)\n",
    "\n",
    "    # Compute consumption and impose non-negative constraint\n",
    "    c_orig = w - ph*(1+delta)*h_out - a_out_all\n",
    "    c = torch.maximum(c_orig, torch.ones_like(c_orig)*lb)\n",
    "\n",
    "    # Compute aggregate savings and housing\n",
    "    A_out = torch.sum(torch.from_numpy(mu).to(device) * a_out_all, dim=1, keepdims=True)\n",
    "    H_out = torch.sum(torch.from_numpy(mu).to(device) * h_out, dim=1, keepdims=True)\n",
    "\n",
    "    return R_orig, R, ph_orig, ph, a_out, a_out_all, h_out_orig, h_out, c_orig, c, A_out, H_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1016]], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[1.0000e-05]], grad_fn=<MaximumBackward0>),\n",
       " tensor([[0.0922]], grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[0.0922]], grad_fn=<MaximumBackward0>),\n",
       " tensor([[0.0256, 0.0772]], grad_fn=<SliceBackward0>),\n",
       " tensor([[ 2.5601e-02,  7.7151e-02, -2.2495e+03]], grad_fn=<CatBackward0>),\n",
       " tensor([[-0.0525,  0.0330,  0.0906]], grad_fn=<SliceBackward0>),\n",
       " tensor([[0.0010, 0.0330, 0.0906]], grad_fn=<MaximumBackward0>),\n",
       " tensor([[2.6608e-01, 2.8812e-01, 2.2500e+03]], grad_fn=<SubBackward0>),\n",
       " tensor([[2.6608e-01, 2.8812e-01, 2.2500e+03]], grad_fn=<MaximumBackward0>),\n",
       " tensor([[-662.3096]], dtype=torch.float64, grad_fn=<SumBackward1>),\n",
       " tensor([[0.0412]], dtype=torch.float64, grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_economy(X, model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_state(X, output):\n",
    "    \"\"\"\n",
    "    Computes the state for the next period, given the outputs of the neural network today.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs : tensors\n",
    "        Outputs of neural network\n",
    "    \"\"\"\n",
    "\n",
    "    _, R, _, ph, a_out, _, _, h_out, _, _, _, _ = compute_economy(X, output)\n",
    "\n",
    "    m = torch.Tensor.size(X)[0]\n",
    "\n",
    "    a_prime = torch.cat([torch.zeros([m, 1]), a_out], dim=1)\n",
    "    h_prime = torch.cat([torch.zeros([m, 1]), h_out[:, 0:J-1]], dim=1)\n",
    "\n",
    "    # Distribution of tomorrow's financial wealth\n",
    "    w_prime = y + (R*a_prime + Pih*ph*h_prime) / (zeta_0*Gchi*Pic)\n",
    "\n",
    "    # Tomorrow's extended state: catenate the parts together\n",
    "    X_prime = torch.cat([\n",
    "        R,\n",
    "        ph,\n",
    "        w_prime,\n",
    "        a_prime,\n",
    "        h_prime\n",
    "        ], dim=1)        \n",
    "\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-05, 9.2223e-02, 8.2221e-01, 1.2334e+00, 2.0388e-01, 0.0000e+00,\n",
       "         2.5601e-02, 7.7151e-02, 0.0000e+00, 1.0000e-03, 3.3003e-02]],\n",
       "       dtype=torch.float64, grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_next_state(X, model(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_period(X):\n",
    "    \"\"\"\n",
    "    Evaluate neural network given today's state and generate tomorrow's state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : tensor\n",
    "        Today's extended state\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        Tomorrow's extended state\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate neural network\n",
    "    output = model(X)\n",
    "\n",
    "    # Compute next state\n",
    "    X_prime = compute_next_state(X, output)\n",
    "    \n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-05, 9.2223e-02, 8.2221e-01, 1.2334e+00, 2.0388e-01, 0.0000e+00,\n",
       "         2.5601e-02, 7.7151e-02, 0.0000e+00, 1.0000e-03, 3.3003e-02]],\n",
       "       dtype=torch.float64, grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_period(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_errors(X, output, X_prime, output_prime):\n",
    "    \"\"\"\n",
    "    Compute error functions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : _type_\n",
    "        _description_\n",
    "    output : _type_\n",
    "        _description_\n",
    "    X_prime : _type_\n",
    "        _description_\n",
    "    output_prime : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    # Compute today's economy\n",
    "    R_orig, R, ph_orig, ph, _, _, h_out_orig, h_out, c_orig, c, A_out, H_out = compute_economy(X, output)\n",
    "\n",
    "    # Compute tomorrow's economy\n",
    "    R_prime_orig, R_prime, ph_prime_orig, ph_prime, _, _, h_prime_out_orig, h_prime_out, c_prime_orig, c_prime, A_prime_out, H_prime_out = compute_economy(X_prime, output_prime)\n",
    "\n",
    "    # Euler equation 1 (Intertemporal)\n",
    "    opt_euler1 = -1 + (c_prime[:, 1:J] / c[:, 0:J-1])*(R*betahat/(Gchi*Pic))**(-1/gamma)\n",
    "    # opt_euler1 = grad_Uc(c[:, 0:J-1], h_out[:, 0:J-1]) - grad_Uc(c_prime[:, 1:J], h_prime_out[:, 1:J]) * R*betahat/(Gchi*Pic)\n",
    "\n",
    "    # Euler equation 2 (Intratemporal)\n",
    "    opt_euler2 = -1 + (c/h_out)*(((1-alpha)/alpha)*ph*(1+delta-Pih/R))**(-1)\n",
    "    # opt_euler_intra = -1 + (c/h_out)*(((1-alpha)/alpha)*ph*(1+delta-Pih/R))**(-1)\n",
    "    # opt_euler_intra_prime = -1 + (c_prime/h_prime_out)*(((1-alpha)/alpha)*ph_prime*(1+delta-Pih/R_prime))**(-1)\n",
    "    # opt_euler2 = torch.cat([opt_euler_intra, opt_euler_intra_prime], dim=1)\n",
    "\n",
    "    # Punishment for negative consumption\n",
    "    orig_cons = torch.cat([c_orig, c_prime_orig], dim=1)\n",
    "    opt_punish_cons = (1./eps) * torch.maximum(-1 * orig_cons, torch.zeros_like(orig_cons))\n",
    "\n",
    "    # Punishment for negative housing\n",
    "    orig_housing = torch.cat([h_out_orig, h_prime_out_orig], dim=1)\n",
    "    opt_punish_housing = (1./eps) * torch.maximum(-1 * orig_housing, torch.zeros_like(orig_housing))\n",
    "\n",
    "    # Punishment for negative interest rate\n",
    "    orig_R = torch.cat([R_orig, R_prime_orig], dim=1)\n",
    "    opt_punish_R = (1./eps) * torch.maximum(-1 * orig_R, torch.zeros_like(orig_R))\n",
    "\n",
    "    # Punishment for negative house price\n",
    "    orig_ph = torch.cat([ph_orig, ph_prime_orig], dim=1)\n",
    "    opt_punish_ph = (1./eps) * torch.maximum(-1 * orig_ph, torch.zeros_like(orig_ph))\n",
    "\n",
    "    # Market clearing condition for aggregate savings\n",
    "    # opt_market_a = A_out\n",
    "    # opt_market_a_prime = A_prime_out\n",
    "    # opt_market_A = torch.cat([opt_market_a, opt_market_a_prime], dim=1)\n",
    "    opt_market_A = A_out\n",
    "\n",
    "    # Market clearing condition for aggregate housing\n",
    "    # opt_market_h = H_out - Hs\n",
    "    # opt_market_h_prime = H_prime_out - Hs\n",
    "    # opt_market_H = torch.cat([opt_market_h, opt_market_h_prime], dim=1)\n",
    "    opt_market_H = H_out - Hs\n",
    "\n",
    "    # Concatenate the equilibrium functions\n",
    "    combined_opt = [opt_euler1, opt_euler2, opt_punish_cons, opt_punish_housing, opt_punish_R, opt_punish_ph, opt_market_A, opt_market_H]\n",
    "    opt_predict = torch.cat(combined_opt, dim=1)\n",
    "\n",
    "    # Define the \"correct\" outputs. For all equilibrium functions, the correct outputs is zero.\n",
    "    opt_correct = torch.zeros_like(opt_predict)\n",
    "\n",
    "    return opt_predict, opt_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(X):\n",
    "    \"\"\"\n",
    "    Simulate and return euler errors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        Euler errors\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate neural network given today's state\n",
    "    output = model(X)\n",
    "\n",
    "    # Compute state tomorrow\n",
    "    X_prime = compute_next_state(X, output)\n",
    "\n",
    "    # Evaluate neural network given tomorrow's state\n",
    "    output_prime = model(X_prime.float())\n",
    "\n",
    "    # Compute euler errors and other equilibrium conditions\n",
    "    opt_predict, opt_correct = euler_errors(X, output, X_prime, output_prime)\n",
    "\n",
    "    return opt_predict, opt_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_predict, opt_correct = simulate(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1640e+09, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(opt_predict, opt_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random economically feasible starting point\n",
    "x_start = torch.rand(size=(1, input_size))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, capturable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episodes(x_start, episode_length):\n",
    "    \"\"\"Simulate an episode for a given starting point using the current\n",
    "       neural network state.\n",
    "\n",
    "    Args:\n",
    "        x_start: Starting state to simulate forward from,\n",
    "        episode_length: Number of steps to simulate forward,\n",
    "\n",
    "    Returns:\n",
    "        X_episodes: Tensor of states [z, k] to train on (training set).\n",
    "    \"\"\"\n",
    "    \n",
    "    dim_state = torch.Tensor.size(x_start)[1]\n",
    "\n",
    "    # Initialise empty array of episodes\n",
    "    X_episodes = torch.zeros([episode_length, dim_state])\n",
    "    X_episodes[0, :] = x_start\n",
    "    X_old = x_start\n",
    "\n",
    "    for t in range(1, episode_length):\n",
    "        X_new = run_period(X_old.float())\n",
    "        \n",
    "        # Append it to the dataset\n",
    "        X_episodes[t, :] = X_new\n",
    "        X_old = X_new\n",
    "\n",
    "    return X_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.9295e-01,  2.3815e-02,  4.9834e-01,  4.8247e-01,  6.2488e-01,\n",
       "          9.9187e-01,  5.1755e-01,  8.8799e-01,  1.2746e-01,  4.3731e-01,\n",
       "          2.2084e-01],\n",
       "        [ 1.0000e-05,  9.7132e-02,  8.2221e-01,  1.2334e+00,  2.0888e-01,\n",
       "          0.0000e+00, -3.5709e-02,  8.7200e-02,  0.0000e+00,  1.0000e-03,\n",
       "          7.1680e-02]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_episodes(x_start, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X_train):\n",
    "\n",
    "    X_train = torch.utils.data.DataLoader(X_train, batch_size=1)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs_per_episode):\n",
    "\n",
    "        for X_batch in X_train:\n",
    "\n",
    "            # Forward pass of the model to get Euler errors\n",
    "            opt_predict, opt_correct = simulate(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.mse_loss(opt_predict, opt_correct)             \n",
    "\n",
    "            # Use gradient tape to retrieve gradients of trainable variables with respect to loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"loss: {torch.log(loss):>7f}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 15.293849\n",
      "loss: 15.287631\n",
      "loss: 15.281349\n",
      "loss: 15.275002\n",
      "loss: 15.268589\n",
      "loss: 15.262127\n",
      "loss: 15.255616\n",
      "loss: 15.249112\n",
      "loss: 15.242569\n",
      "loss: 15.235990\n",
      "loss: 15.229376\n",
      "loss: 15.222729\n",
      "loss: 15.216137\n",
      "loss: 15.210132\n",
      "loss: 15.204100\n",
      "loss: 15.198044\n",
      "loss: 15.191965\n",
      "loss: 15.185864\n",
      "loss: 15.179743\n",
      "loss: 15.173602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3888765.4719, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(x_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_algorithm(x_start):\n",
    "\n",
    "    num_episodes = 10 #try 30000\n",
    "    loss_list = np.empty(num_episodes)\n",
    "\n",
    "    # Training Loops\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Simulate episodes\n",
    "        X_episodes = simulate_episodes(x_start, len_episodes)\n",
    "\n",
    "        # Train model\n",
    "        loss_value = train_step(X_episodes)\n",
    "\n",
    "        # Update starting episode\n",
    "        x_start = X_episodes[-1, :].reshape([1, -1])\n",
    "\n",
    "        # Store losses, euler errors\n",
    "        loss_list[episode] = torch.log(loss_value)\n",
    "        \n",
    "        # Log\n",
    "        if episode % 1 == 0:\n",
    "            print(f\"Episode {episode}: log10(loss): {torch.log(loss_value):.5f}\")\n",
    "            print(x_start)\n",
    "          \n",
    "    return x_start, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 15.167444\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [50, 7]], which is output 0 of AsStridedBackward0, is at version 193; expected version 192 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Work\\DeepEquilibriumNets\\3g simple pytorch.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000040?line=0'>1</a>\u001b[0m x_final, loss_list \u001b[39m=\u001b[39m training_algorithm(x_start)\n",
      "\u001b[1;32md:\\Documents\\Work\\DeepEquilibriumNets\\3g simple pytorch.ipynb Cell 27'\u001b[0m in \u001b[0;36mtraining_algorithm\u001b[1;34m(x_start)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000039?line=9'>10</a>\u001b[0m X_episodes \u001b[39m=\u001b[39m simulate_episodes(x_start, len_episodes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000039?line=11'>12</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000039?line=12'>13</a>\u001b[0m loss_value \u001b[39m=\u001b[39m train_step(X_episodes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000039?line=14'>15</a>\u001b[0m \u001b[39m# Update starting episode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000039?line=15'>16</a>\u001b[0m x_start \u001b[39m=\u001b[39m X_episodes[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mreshape([\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32md:\\Documents\\Work\\DeepEquilibriumNets\\3g simple pytorch.ipynb Cell 25'\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(X_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000036?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mmse_loss(opt_predict, opt_correct)             \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000036?line=15'>16</a>\u001b[0m \u001b[39m# Use gradient tape to retrieve gradients of trainable variables with respect to loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000036?line=16'>17</a>\u001b[0m \u001b[39m# optimizer.zero_grad()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000036?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000036?line=18'>19</a>\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Work/DeepEquilibriumNets/3g%20simple%20pytorch.ipynb#ch0000036?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Harry\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [50, 7]], which is output 0 of AsStridedBackward0, is at version 193; expected version 192 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "x_final, loss_list = training_algorithm(x_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4853306a52d5055eb45ccebd398f44d73d5814f856fe5205a72cd2003d3ca148"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
